{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, and more. Looking for a quick read through how the core features are used? Check out this detailed blog post with a practical example. Tenets \u00b6 This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic . Utilities follow programming language idioms and language-specific best practices. Install \u00b6 Powertools is available in PyPi. You can use your favourite dependency management tool to install it poetry : poetry add aws-lambda-powertools pip : pip install aws-lambda-powertools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python Lambda Layer \u00b6 Powertools is also available as a Lambda Layer, and it is distributed via the AWS Serverless Application Repository (SAR) to support semantic versioning. App ARN Description aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Core dependencies only; sufficient for nearly all utilities. aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras Core plus extra dependencies such as pydantic that is required by parser utility. Warning Layer-extras does not support Python 3.6 runtime. This layer also includes all extra dependencies: 22.4MB zipped , ~155MB unzipped . If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.10.2 # change to latest semantic version available in SAR MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : # fetch Layer ARN from SAR App stack output - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Example of least-privileged IAM permissions to deploy Layer Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 AWSTemplateFormatVersion : \"2010-09-09\" Resources : PowertoolsLayerIamRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"cloudformation.amazonaws.com\" Action : - \"sts:AssumeRole\" Path : \"/\" PowertoolsLayerIamPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : PowertoolsLambdaLayerPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.10.2/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:layer:aws-lambda-powertools-python-layer* Roles : - Ref : \"PowertoolsLayerIamRole\" You can fetch available versions via SAR API with: 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Features \u00b6 Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Middleware factory Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Batch processing Handle partial failures for AWS SQS batch processing Typing Static typing classes to speedup development in your IDE Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Parser Data parsing and deep validation using Pydantic Idempotency Idempotent Lambda handler Environment variables \u00b6 Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO Debug mode \u00b6 As a best practice, AWS Lambda Powertools logging statements are suppressed. If necessary, you can enable debugging using set_package_logger : app.py 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger ()","title":"Homepage"},{"location":"#tenets","text":"This project separates core utilities that will be available in other runtimes vs general utilities that might not be available across all runtimes. AWS Lambda only . We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices . The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean . Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility . New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community . We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic . Utilities follow programming language idioms and language-specific best practices.","title":"Tenets"},{"location":"#install","text":"Powertools is available in PyPi. You can use your favourite dependency management tool to install it poetry : poetry add aws-lambda-powertools pip : pip install aws-lambda-powertools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python","title":"Install"},{"location":"#lambda-layer","text":"Powertools is also available as a Lambda Layer, and it is distributed via the AWS Serverless Application Repository (SAR) to support semantic versioning. App ARN Description aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Core dependencies only; sufficient for nearly all utilities. aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras Core plus extra dependencies such as pydantic that is required by parser utility. Warning Layer-extras does not support Python 3.6 runtime. This layer also includes all extra dependencies: 22.4MB zipped , ~155MB unzipped . If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.10.2 # change to latest semantic version available in SAR MyLambdaFunction : Type : AWS::Serverless::Function Properties : Layers : # fetch Layer ARN from SAR App stack output - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Example of least-privileged IAM permissions to deploy Layer Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 AWSTemplateFormatVersion : \"2010-09-09\" Resources : PowertoolsLayerIamRole : Type : \"AWS::IAM::Role\" Properties : AssumeRolePolicyDocument : Version : \"2012-10-17\" Statement : - Effect : \"Allow\" Principal : Service : - \"cloudformation.amazonaws.com\" Action : - \"sts:AssumeRole\" Path : \"/\" PowertoolsLayerIamPolicy : Type : \"AWS::IAM::Policy\" Properties : PolicyName : PowertoolsLambdaLayerPolicy PolicyDocument : Version : \"2012-10-17\" Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.10.2/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:layer:aws-lambda-powertools-python-layer* Roles : - Ref : \"PowertoolsLayerIamRole\" You can fetch available versions via SAR API with: 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer","title":"Lambda Layer"},{"location":"#features","text":"Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logger Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Middleware factory Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Batch processing Handle partial failures for AWS SQS batch processing Typing Static typing classes to speedup development in your IDE Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Parser Data parsing and deep validation using Pydantic Idempotency Idempotent Lambda handler","title":"Features"},{"location":"#environment-variables","text":"Info Explicit parameters take precedence over environment variables. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO","title":"Environment variables"},{"location":"#debug-mode","text":"As a best practice, AWS Lambda Powertools logging statements are suppressed. If necessary, you can enable debugging using set_package_logger : app.py 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger ()","title":"Debug mode"},{"location":"changelog/","text":"Changelog \u00b6 All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning . [Unreleased] \u00b6 [1.12.0] - 2021-03-17 \u00b6 Added \u00b6 Parameters : New force_fetch param to always fetch the latest and bypass cache, if available Data Classes : New AppSync Lambda Resolver event covering both Direct Lambda Resolver and Amplify GraphQL Transformer Resolver @function Data Classes : New AppSync scalar utilities to easily compose Lambda Resolvers with date utils, uuid, etc. Logger : Support for Correlation ID both in inject_lambda_context decorator and set_correlation_id method Logger : Include new exception_name key to help customers easily enumerate exceptions across all functions Fixed \u00b6 Tracer : Type hint on return instance that made PyCharm no longer recognize autocompletion Idempotency : Error handling for missing idempotency key and save_in_progress errors [1.11.0] - 2021-03-05 \u00b6 Fixed \u00b6 Tracer : Lazy loads X-Ray SDK to increase perf by 75% for those not instantiating Tracer Metrics : Optimize validation and serialization to increase perf by nearly 50% for large operations (<1ms) Added \u00b6 Dataclass : Add new Amazon Connect contact flow event Idempotency : New Idempotency utility Docs : Add example on how to integrate Batch utility with Sentry.io Internal : Added performance SLA tests for high level imports and Metrics validation/serialization [1.10.5] - 2021-02-17 \u00b6 No changes. Bumped version to trigger new pipeline build for layer publishing. [1.10.4] - 2021-02-17 \u00b6 Fixed \u00b6 Docs : Fix anchor tags to be lower case Docs : Correct the docs location for the labeller [1.10.3] - 2021-02-04 \u00b6 Added \u00b6 Docs : Migrated from Gatsby to MKdocs documentation system Docs : Included Getting started and Advanced sections in Core utilities, including additional examples Fixed \u00b6 Tracer : Disabled batching segments as X-Ray SDK does not flush traces upon reaching limits Parser : Model type is now compliant with mypy [1.10.2] - 2021-02-04 \u00b6 Fixed \u00b6 Utilities : Correctly handle and list multiple exceptions in SQS batch processing utility. * Docs :: Fix typos on AppConfig docstring import, and SnsModel typo in parser. Utilities : typing_extensions package is now only installed in Python < 3.8 [1.10.1] - 2021-01-19 \u00b6 Fixed \u00b6 Utilities : Added SnsSqsEnvelope in parser to dynamically adjust model mismatch when customers use SNS + SQS instead of SNS + Lambda, since we've discovered three payload keys are slightly different. [1.10.0] - 2021-01-18 \u00b6 Added \u00b6 Utilities : Added support for AppConfig in Parameters utility Logger : Added support for extra parameter to add additional root fields when logging messages Logger : Added support to Pytest Live Log feat. via feature toggle POWERTOOLS_LOG_DEDUPLICATION_DISABLED Tracer : Added support to disable auto-capturing response and exception as metadata Utilities : Added support to handle custom string/integer formats in JSON Schema in Validator utility Install : Added new Lambda Layer with all extra dependencies installed, available in Serverless Application Repository (SAR) Fixed \u00b6 Docs : Added missing SNS parser model Docs : Added new environment variables for toggling features in Logger and Tracer: POWERTOOLS_LOG_DEDUPLICATION_DISABLED , POWERTOOLS_TRACER_CAPTURE_RESPONSE , POWERTOOLS_TRACER_CAPTURE_ERROR Docs : Fixed incorrect import for Cognito data classes in Event Sources utility [1.9.1] - 2020-12-21 \u00b6 Fixed \u00b6 Logger : Bugfix to prevent parent loggers with the same name being configured more than once Added \u00b6 Docs : Add clarification to Tracer docs for how capture_method decorator can cause function responses to be read and serialized. Utilities : Added equality to ease testing Event source data classes Package : Added py.typed for initial work needed for PEP 561 compliance [1.9.0] - 2020-12-04 \u00b6 Added \u00b6 Utilities : Added Kinesis, S3, CloudWatch Logs, Application Load Balancer, and SES support in Parser Docs : Sidebar menu are now always expanded Fixed \u00b6 Docs : Broken link to GitHub to homepage [1.8.0] - 2020-11-20 \u00b6 Added \u00b6 Utilities : Added support for new EventBridge Replay field in Parser and Event source data classes Utilities : Added SNS support in Parser Utilities : Added API Gateway HTTP API data class support for new IAM and Lambda authorizer in Event source data classes Docs : Add new FAQ section for Logger on how to enable debug logging for boto3 Docs : Add explicit minimal set of permissions required to use Layers provided by Serverless Application Repository (SAR) Fixed \u00b6 Docs : Fix typo in Dataclasses example for SES when fetching common email headers [1.7.0] - 2020-10-26 \u00b6 Added \u00b6 Utilities : Add new Parser utility to provide parsing and deep data validation using Pydantic Models Utilities : Add case insensitive header lookup, and Cognito custom auth triggers to Event source data classes Fixed \u00b6 Logger : keeps Lambda root logger handler, and add log filter instead to prevent child log records duplication Docs : Improve wording on adding log keys conditionally [1.6.1] - 2020-09-23 \u00b6 Fixed \u00b6 Utilities : Fix issue with boolean values in DynamoDB stream event data class. [1.6.0] - 2020-09-22 \u00b6 Added \u00b6 Metrics : Support adding multiple metric values to a single metric name Utilities : Add new Validator utility to validate inbound events and responses using JSON Schema Utilities : Add new Event source data classes utility to easily describe event schema of popular event sources Docs : Add new Testing your code section to both Logger and Metrics page, and content width is now wider Tracer : Support for automatically disable Tracer when running a Chalice app Fixed \u00b6 Docs : Improve wording on log sampling feature in Logger, and removed duplicate content on main page Utilities : Remove DeleteMessageBatch API call when there are no messages to delete [1.5.0] - 2020-09-04 \u00b6 Added \u00b6 Logger : Add xray_trace_id to log output to improve integration with CloudWatch Service Lens Logger : Allow reordering of logged output Utilities : Add new SQS batch processing utility to handle partial failures in processing message batches Utilities : Add typing utility providing static type for lambda context object Utilities : Add transform=auto in parameters utility to deserialize parameter values based on the key name Fixed \u00b6 Logger : The value of json_default formatter is no longer written to logs [1.4.0] - 2020-08-25 \u00b6 Added \u00b6 All : Official Lambda Layer via Serverless Application Repository Tracer : capture_method and capture_lambda_handler now support capture_response=False parameter to prevent Tracer to capture response as metadata to allow customers running Tracer with sensitive workloads Fixed \u00b6 Metrics : Cold start metric is now completely separate from application metrics dimensions, making it easier and cheaper to visualize. This is a breaking change if you were graphing/alerting on both application metrics with the same name to compensate this previous malfunctioning Marked as bugfix as this is the intended behaviour since the beginning, as you shouldn't have the same application metric with different dimensions Utilities : SSMProvider within Parameters utility now have decrypt and recursive parameters correctly defined to support autocompletion Added \u00b6 Tracer : capture_lambda_handler and capture_method decorators now support capture_response parameter to not include function's response as part of tracing metadata [1.3.1] - 2020-08-22 \u00b6 Fixed \u00b6 Tracer : capture_method decorator did not properly handle nested context managers [1.3.0] - 2020-08-21 \u00b6 Added \u00b6 Utilities : Add new parameters utility to retrieve a single or multiple parameters from SSM Parameter Store, Secrets Manager, DynamoDB, or your very own [1.2.0] - 2020-08-20 \u00b6 Added \u00b6 Tracer : capture_method decorator now supports generator functions (including context managers) [1.1.3] - 2020-08-18 \u00b6 Fixed \u00b6 Logger : Logs emitted twice, structured and unstructured, due to Lambda configuring the root handler [1.1.2] - 2020-08-16 \u00b6 Fixed \u00b6 Docs : Clarify confusion on Tracer reuse and auto_patch=False statement Logger : Autocomplete for log statements in PyCharm [1.1.1] - 2020-08-14 \u00b6 Fixed \u00b6 Logger : Regression on Logger level not accepting int i.e. Logger(level=logging.INFO) [1.1.0] - 2020-08-14 \u00b6 Added \u00b6 Logger : Support for logger inheritance with child parameter Fixed \u00b6 Logger : Log level is now case insensitive via params and env var [1.0.2] - 2020-07-16 \u00b6 Fixed \u00b6 Tracer : Correct AWS X-Ray SDK dependency to support 2.5.0 and higher [1.0.1] - 2020-07-06 \u00b6 Fixed \u00b6 Logger : Fix a bug with inject_lambda_context causing existing Logger keys to be overridden if structure_logs was called before [1.0.0] - 2020-06-18 \u00b6 Added \u00b6 Metrics : add_metadata method to add any metric metadata you'd like to ease finding metric related data via CloudWatch Logs Set status as General Availability [0.11.0] - 2020-06-08 \u00b6 Added \u00b6 Imports can now be made from top level of module, e.g.: from aws_lambda_powertools import Logger, Metrics, Tracer Fixed \u00b6 Metrics : Fix a bug with Metrics causing an exception to be thrown when logging metrics if dimensions were not explicitly added. Changed \u00b6 Metrics : No longer throws exception by default in case no metrics are emitted when using the log_metrics decorator. [0.10.0] - 2020-06-08 \u00b6 Added \u00b6 Metrics : capture_cold_start_metric parameter added to log_metrics decorator Metrics : Optional namespace and service parameters added to Metrics constructor to more closely resemble other core utils Changed \u00b6 Metrics : Default dimension is now created based on service parameter or POWERTOOLS_SERVICE_NAME env var Deprecated \u00b6 Metrics : add_namespace method deprecated in favor of using namespace parameter to Metrics constructor or POWERTOOLS_METRICS_NAMESPACE env var [0.9.5] - 2020-06-02 \u00b6 Fixed \u00b6 Metrics : Coerce non-string dimension values to string Logger : Correct cold_start , function_memory_size values from string to bool and int respectively [0.9.4] - 2020-05-29 \u00b6 Fixed \u00b6 Metrics : Fix issue where metrics were not correctly flushed, and cleared on every invocation [0.9.3] - 2020-05-16 \u00b6 Fixed \u00b6 Tracer : Fix Runtime Error for nested sync due to incorrect loop usage [0.9.2] - 2020-05-14 \u00b6 Fixed \u00b6 Tracer : Import aiohttp lazily so it's not a hard dependency [0.9.0] - 2020-05-12 \u00b6 Added \u00b6 Tracer : Support for async functions in Tracer via capture_method decorator Tracer : Support for aiohttp via aiohttp_trace_config trace config Tracer : Support for patching specific modules via patch_modules param Tracer : Document escape hatch mechanisms via tracer.provider [0.8.1] - 2020-05-1 \u00b6 Fixed \u00b6 Metrics : Fix metric unit casting logic if one passes plain string (value or key) Metrics: : Fix MetricUnit enum values for BytesPerSecond KilobytesPerSecond MegabytesPerSecond GigabytesPerSecond TerabytesPerSecond BitsPerSecond KilobitsPerSecond MegabitsPerSecond GigabitsPerSecond TerabitsPerSecond CountPerSecond [0.8.0] - 2020-04-24 \u00b6 Added \u00b6 Logger : Introduced Logger class for structured logging as a replacement for logger_setup Logger : Introduced Logger.inject_lambda_context decorator as a replacement for logger_inject_lambda_context Removed \u00b6 Logger : Raise DeprecationWarning exception for both logger_setup , logger_inject_lambda_context [0.7.0] - 2020-04-20 \u00b6 Added \u00b6 Middleware factory : Introduced Middleware Factory to build your own middleware via lambda_handler_decorator Fixed \u00b6 Metrics : Fixed metrics dimensions not being included correctly in EMF [0.6.3] - 2020-04-09 \u00b6 Fixed \u00b6 Logger : Fix log_metrics decorator logic not calling the decorated function, and exception handling [0.6.1] - 2020-04-08 \u00b6 Added \u00b6 Metrics : Introduces Metrics middleware to utilise CloudWatch Embedded Metric Format Deprecated \u00b6 Metrics : Added deprecation warning for log_metrics [0.5.0] - 2020-02-20 \u00b6 Added \u00b6 Logger : Introduced log sampling for debug - Thanks to Danilo's contribution [0.1.0] - 2019-11-15 \u00b6 Added \u00b6 Public beta release","title":"Changelog"},{"location":"changelog/#changelog","text":"All notable changes to this project will be documented in this file. This project follows Keep a Changelog format for changes and adheres to Semantic Versioning .","title":"Changelog"},{"location":"changelog/#unreleased","text":"","title":"[Unreleased]"},{"location":"changelog/#1120-2021-03-17","text":"","title":"[1.12.0] - 2021-03-17"},{"location":"changelog/#added","text":"Parameters : New force_fetch param to always fetch the latest and bypass cache, if available Data Classes : New AppSync Lambda Resolver event covering both Direct Lambda Resolver and Amplify GraphQL Transformer Resolver @function Data Classes : New AppSync scalar utilities to easily compose Lambda Resolvers with date utils, uuid, etc. Logger : Support for Correlation ID both in inject_lambda_context decorator and set_correlation_id method Logger : Include new exception_name key to help customers easily enumerate exceptions across all functions","title":"Added"},{"location":"changelog/#fixed","text":"Tracer : Type hint on return instance that made PyCharm no longer recognize autocompletion Idempotency : Error handling for missing idempotency key and save_in_progress errors","title":"Fixed"},{"location":"changelog/#1110-2021-03-05","text":"","title":"[1.11.0] - 2021-03-05"},{"location":"changelog/#fixed_1","text":"Tracer : Lazy loads X-Ray SDK to increase perf by 75% for those not instantiating Tracer Metrics : Optimize validation and serialization to increase perf by nearly 50% for large operations (<1ms)","title":"Fixed"},{"location":"changelog/#added_1","text":"Dataclass : Add new Amazon Connect contact flow event Idempotency : New Idempotency utility Docs : Add example on how to integrate Batch utility with Sentry.io Internal : Added performance SLA tests for high level imports and Metrics validation/serialization","title":"Added"},{"location":"changelog/#1105-2021-02-17","text":"No changes. Bumped version to trigger new pipeline build for layer publishing.","title":"[1.10.5] - 2021-02-17"},{"location":"changelog/#1104-2021-02-17","text":"","title":"[1.10.4] - 2021-02-17"},{"location":"changelog/#fixed_2","text":"Docs : Fix anchor tags to be lower case Docs : Correct the docs location for the labeller","title":"Fixed"},{"location":"changelog/#1103-2021-02-04","text":"","title":"[1.10.3] - 2021-02-04"},{"location":"changelog/#added_2","text":"Docs : Migrated from Gatsby to MKdocs documentation system Docs : Included Getting started and Advanced sections in Core utilities, including additional examples","title":"Added"},{"location":"changelog/#fixed_3","text":"Tracer : Disabled batching segments as X-Ray SDK does not flush traces upon reaching limits Parser : Model type is now compliant with mypy","title":"Fixed"},{"location":"changelog/#1102-2021-02-04","text":"","title":"[1.10.2] - 2021-02-04"},{"location":"changelog/#fixed_4","text":"Utilities : Correctly handle and list multiple exceptions in SQS batch processing utility. * Docs :: Fix typos on AppConfig docstring import, and SnsModel typo in parser. Utilities : typing_extensions package is now only installed in Python < 3.8","title":"Fixed"},{"location":"changelog/#1101-2021-01-19","text":"","title":"[1.10.1] - 2021-01-19"},{"location":"changelog/#fixed_5","text":"Utilities : Added SnsSqsEnvelope in parser to dynamically adjust model mismatch when customers use SNS + SQS instead of SNS + Lambda, since we've discovered three payload keys are slightly different.","title":"Fixed"},{"location":"changelog/#1100-2021-01-18","text":"","title":"[1.10.0] - 2021-01-18"},{"location":"changelog/#added_3","text":"Utilities : Added support for AppConfig in Parameters utility Logger : Added support for extra parameter to add additional root fields when logging messages Logger : Added support to Pytest Live Log feat. via feature toggle POWERTOOLS_LOG_DEDUPLICATION_DISABLED Tracer : Added support to disable auto-capturing response and exception as metadata Utilities : Added support to handle custom string/integer formats in JSON Schema in Validator utility Install : Added new Lambda Layer with all extra dependencies installed, available in Serverless Application Repository (SAR)","title":"Added"},{"location":"changelog/#fixed_6","text":"Docs : Added missing SNS parser model Docs : Added new environment variables for toggling features in Logger and Tracer: POWERTOOLS_LOG_DEDUPLICATION_DISABLED , POWERTOOLS_TRACER_CAPTURE_RESPONSE , POWERTOOLS_TRACER_CAPTURE_ERROR Docs : Fixed incorrect import for Cognito data classes in Event Sources utility","title":"Fixed"},{"location":"changelog/#191-2020-12-21","text":"","title":"[1.9.1] - 2020-12-21"},{"location":"changelog/#fixed_7","text":"Logger : Bugfix to prevent parent loggers with the same name being configured more than once","title":"Fixed"},{"location":"changelog/#added_4","text":"Docs : Add clarification to Tracer docs for how capture_method decorator can cause function responses to be read and serialized. Utilities : Added equality to ease testing Event source data classes Package : Added py.typed for initial work needed for PEP 561 compliance","title":"Added"},{"location":"changelog/#190-2020-12-04","text":"","title":"[1.9.0] - 2020-12-04"},{"location":"changelog/#added_5","text":"Utilities : Added Kinesis, S3, CloudWatch Logs, Application Load Balancer, and SES support in Parser Docs : Sidebar menu are now always expanded","title":"Added"},{"location":"changelog/#fixed_8","text":"Docs : Broken link to GitHub to homepage","title":"Fixed"},{"location":"changelog/#180-2020-11-20","text":"","title":"[1.8.0] - 2020-11-20"},{"location":"changelog/#added_6","text":"Utilities : Added support for new EventBridge Replay field in Parser and Event source data classes Utilities : Added SNS support in Parser Utilities : Added API Gateway HTTP API data class support for new IAM and Lambda authorizer in Event source data classes Docs : Add new FAQ section for Logger on how to enable debug logging for boto3 Docs : Add explicit minimal set of permissions required to use Layers provided by Serverless Application Repository (SAR)","title":"Added"},{"location":"changelog/#fixed_9","text":"Docs : Fix typo in Dataclasses example for SES when fetching common email headers","title":"Fixed"},{"location":"changelog/#170-2020-10-26","text":"","title":"[1.7.0] - 2020-10-26"},{"location":"changelog/#added_7","text":"Utilities : Add new Parser utility to provide parsing and deep data validation using Pydantic Models Utilities : Add case insensitive header lookup, and Cognito custom auth triggers to Event source data classes","title":"Added"},{"location":"changelog/#fixed_10","text":"Logger : keeps Lambda root logger handler, and add log filter instead to prevent child log records duplication Docs : Improve wording on adding log keys conditionally","title":"Fixed"},{"location":"changelog/#161-2020-09-23","text":"","title":"[1.6.1] - 2020-09-23"},{"location":"changelog/#fixed_11","text":"Utilities : Fix issue with boolean values in DynamoDB stream event data class.","title":"Fixed"},{"location":"changelog/#160-2020-09-22","text":"","title":"[1.6.0] - 2020-09-22"},{"location":"changelog/#added_8","text":"Metrics : Support adding multiple metric values to a single metric name Utilities : Add new Validator utility to validate inbound events and responses using JSON Schema Utilities : Add new Event source data classes utility to easily describe event schema of popular event sources Docs : Add new Testing your code section to both Logger and Metrics page, and content width is now wider Tracer : Support for automatically disable Tracer when running a Chalice app","title":"Added"},{"location":"changelog/#fixed_12","text":"Docs : Improve wording on log sampling feature in Logger, and removed duplicate content on main page Utilities : Remove DeleteMessageBatch API call when there are no messages to delete","title":"Fixed"},{"location":"changelog/#150-2020-09-04","text":"","title":"[1.5.0] - 2020-09-04"},{"location":"changelog/#added_9","text":"Logger : Add xray_trace_id to log output to improve integration with CloudWatch Service Lens Logger : Allow reordering of logged output Utilities : Add new SQS batch processing utility to handle partial failures in processing message batches Utilities : Add typing utility providing static type for lambda context object Utilities : Add transform=auto in parameters utility to deserialize parameter values based on the key name","title":"Added"},{"location":"changelog/#fixed_13","text":"Logger : The value of json_default formatter is no longer written to logs","title":"Fixed"},{"location":"changelog/#140-2020-08-25","text":"","title":"[1.4.0] - 2020-08-25"},{"location":"changelog/#added_10","text":"All : Official Lambda Layer via Serverless Application Repository Tracer : capture_method and capture_lambda_handler now support capture_response=False parameter to prevent Tracer to capture response as metadata to allow customers running Tracer with sensitive workloads","title":"Added"},{"location":"changelog/#fixed_14","text":"Metrics : Cold start metric is now completely separate from application metrics dimensions, making it easier and cheaper to visualize. This is a breaking change if you were graphing/alerting on both application metrics with the same name to compensate this previous malfunctioning Marked as bugfix as this is the intended behaviour since the beginning, as you shouldn't have the same application metric with different dimensions Utilities : SSMProvider within Parameters utility now have decrypt and recursive parameters correctly defined to support autocompletion","title":"Fixed"},{"location":"changelog/#added_11","text":"Tracer : capture_lambda_handler and capture_method decorators now support capture_response parameter to not include function's response as part of tracing metadata","title":"Added"},{"location":"changelog/#131-2020-08-22","text":"","title":"[1.3.1] - 2020-08-22"},{"location":"changelog/#fixed_15","text":"Tracer : capture_method decorator did not properly handle nested context managers","title":"Fixed"},{"location":"changelog/#130-2020-08-21","text":"","title":"[1.3.0] - 2020-08-21"},{"location":"changelog/#added_12","text":"Utilities : Add new parameters utility to retrieve a single or multiple parameters from SSM Parameter Store, Secrets Manager, DynamoDB, or your very own","title":"Added"},{"location":"changelog/#120-2020-08-20","text":"","title":"[1.2.0] - 2020-08-20"},{"location":"changelog/#added_13","text":"Tracer : capture_method decorator now supports generator functions (including context managers)","title":"Added"},{"location":"changelog/#113-2020-08-18","text":"","title":"[1.1.3] - 2020-08-18"},{"location":"changelog/#fixed_16","text":"Logger : Logs emitted twice, structured and unstructured, due to Lambda configuring the root handler","title":"Fixed"},{"location":"changelog/#112-2020-08-16","text":"","title":"[1.1.2] - 2020-08-16"},{"location":"changelog/#fixed_17","text":"Docs : Clarify confusion on Tracer reuse and auto_patch=False statement Logger : Autocomplete for log statements in PyCharm","title":"Fixed"},{"location":"changelog/#111-2020-08-14","text":"","title":"[1.1.1] - 2020-08-14"},{"location":"changelog/#fixed_18","text":"Logger : Regression on Logger level not accepting int i.e. Logger(level=logging.INFO)","title":"Fixed"},{"location":"changelog/#110-2020-08-14","text":"","title":"[1.1.0] - 2020-08-14"},{"location":"changelog/#added_14","text":"Logger : Support for logger inheritance with child parameter","title":"Added"},{"location":"changelog/#fixed_19","text":"Logger : Log level is now case insensitive via params and env var","title":"Fixed"},{"location":"changelog/#102-2020-07-16","text":"","title":"[1.0.2] - 2020-07-16"},{"location":"changelog/#fixed_20","text":"Tracer : Correct AWS X-Ray SDK dependency to support 2.5.0 and higher","title":"Fixed"},{"location":"changelog/#101-2020-07-06","text":"","title":"[1.0.1] - 2020-07-06"},{"location":"changelog/#fixed_21","text":"Logger : Fix a bug with inject_lambda_context causing existing Logger keys to be overridden if structure_logs was called before","title":"Fixed"},{"location":"changelog/#100-2020-06-18","text":"","title":"[1.0.0] - 2020-06-18"},{"location":"changelog/#added_15","text":"Metrics : add_metadata method to add any metric metadata you'd like to ease finding metric related data via CloudWatch Logs Set status as General Availability","title":"Added"},{"location":"changelog/#0110-2020-06-08","text":"","title":"[0.11.0] - 2020-06-08"},{"location":"changelog/#added_16","text":"Imports can now be made from top level of module, e.g.: from aws_lambda_powertools import Logger, Metrics, Tracer","title":"Added"},{"location":"changelog/#fixed_22","text":"Metrics : Fix a bug with Metrics causing an exception to be thrown when logging metrics if dimensions were not explicitly added.","title":"Fixed"},{"location":"changelog/#changed","text":"Metrics : No longer throws exception by default in case no metrics are emitted when using the log_metrics decorator.","title":"Changed"},{"location":"changelog/#0100-2020-06-08","text":"","title":"[0.10.0] - 2020-06-08"},{"location":"changelog/#added_17","text":"Metrics : capture_cold_start_metric parameter added to log_metrics decorator Metrics : Optional namespace and service parameters added to Metrics constructor to more closely resemble other core utils","title":"Added"},{"location":"changelog/#changed_1","text":"Metrics : Default dimension is now created based on service parameter or POWERTOOLS_SERVICE_NAME env var","title":"Changed"},{"location":"changelog/#deprecated","text":"Metrics : add_namespace method deprecated in favor of using namespace parameter to Metrics constructor or POWERTOOLS_METRICS_NAMESPACE env var","title":"Deprecated"},{"location":"changelog/#095-2020-06-02","text":"","title":"[0.9.5] - 2020-06-02"},{"location":"changelog/#fixed_23","text":"Metrics : Coerce non-string dimension values to string Logger : Correct cold_start , function_memory_size values from string to bool and int respectively","title":"Fixed"},{"location":"changelog/#094-2020-05-29","text":"","title":"[0.9.4] - 2020-05-29"},{"location":"changelog/#fixed_24","text":"Metrics : Fix issue where metrics were not correctly flushed, and cleared on every invocation","title":"Fixed"},{"location":"changelog/#093-2020-05-16","text":"","title":"[0.9.3] - 2020-05-16"},{"location":"changelog/#fixed_25","text":"Tracer : Fix Runtime Error for nested sync due to incorrect loop usage","title":"Fixed"},{"location":"changelog/#092-2020-05-14","text":"","title":"[0.9.2] - 2020-05-14"},{"location":"changelog/#fixed_26","text":"Tracer : Import aiohttp lazily so it's not a hard dependency","title":"Fixed"},{"location":"changelog/#090-2020-05-12","text":"","title":"[0.9.0] - 2020-05-12"},{"location":"changelog/#added_18","text":"Tracer : Support for async functions in Tracer via capture_method decorator Tracer : Support for aiohttp via aiohttp_trace_config trace config Tracer : Support for patching specific modules via patch_modules param Tracer : Document escape hatch mechanisms via tracer.provider","title":"Added"},{"location":"changelog/#081-2020-05-1","text":"","title":"[0.8.1] - 2020-05-1"},{"location":"changelog/#fixed_27","text":"Metrics : Fix metric unit casting logic if one passes plain string (value or key) Metrics: : Fix MetricUnit enum values for BytesPerSecond KilobytesPerSecond MegabytesPerSecond GigabytesPerSecond TerabytesPerSecond BitsPerSecond KilobitsPerSecond MegabitsPerSecond GigabitsPerSecond TerabitsPerSecond CountPerSecond","title":"Fixed"},{"location":"changelog/#080-2020-04-24","text":"","title":"[0.8.0] - 2020-04-24"},{"location":"changelog/#added_19","text":"Logger : Introduced Logger class for structured logging as a replacement for logger_setup Logger : Introduced Logger.inject_lambda_context decorator as a replacement for logger_inject_lambda_context","title":"Added"},{"location":"changelog/#removed","text":"Logger : Raise DeprecationWarning exception for both logger_setup , logger_inject_lambda_context","title":"Removed"},{"location":"changelog/#070-2020-04-20","text":"","title":"[0.7.0] - 2020-04-20"},{"location":"changelog/#added_20","text":"Middleware factory : Introduced Middleware Factory to build your own middleware via lambda_handler_decorator","title":"Added"},{"location":"changelog/#fixed_28","text":"Metrics : Fixed metrics dimensions not being included correctly in EMF","title":"Fixed"},{"location":"changelog/#063-2020-04-09","text":"","title":"[0.6.3] - 2020-04-09"},{"location":"changelog/#fixed_29","text":"Logger : Fix log_metrics decorator logic not calling the decorated function, and exception handling","title":"Fixed"},{"location":"changelog/#061-2020-04-08","text":"","title":"[0.6.1] - 2020-04-08"},{"location":"changelog/#added_21","text":"Metrics : Introduces Metrics middleware to utilise CloudWatch Embedded Metric Format","title":"Added"},{"location":"changelog/#deprecated_1","text":"Metrics : Added deprecation warning for log_metrics","title":"Deprecated"},{"location":"changelog/#050-2020-02-20","text":"","title":"[0.5.0] - 2020-02-20"},{"location":"changelog/#added_22","text":"Logger : Introduced log sampling for debug - Thanks to Danilo's contribution","title":"Added"},{"location":"changelog/#010-2019-11-15","text":"","title":"[0.1.0] - 2019-11-15"},{"location":"changelog/#added_23","text":"Public beta release","title":"Added"},{"location":"core/logger/","text":"Logger provides an opinionated logger with output structured as JSON. Key features \u00b6 Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time Getting started \u00b6 Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example app.py 1 2 3 from aws_lambda_powertools import Logger logger = Logger () # Sets service via env var # OR logger = Logger(service=\"example\") Standard structured keys \u00b6 Your Logger will include the following keys to your structured logging, by default: Key Type Example Description timestamp str \"2020-05-24 18:17:33,774\" Timestamp of actual log statement level str \"INFO\" Logging level location str \"collect.handler:1\" Source code location where statement was executed service str \"payment\" Service name defined. \"service_undefined\" will be used if unknown sampling_rate int 0.1 Debug logging sampling rate in percentage e.g. 10% in this case message any \"Collecting payment\" Log statement value. Unserializable JSON values will be casted to string xray_trace_id str \"1-5759e988-bd862e3fe1be46a994272793\" X-Ray Trace ID when Lambda function has enabled Tracing Capturing Lambda context info \u00b6 You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : \"Collecting payment\" }, { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:15\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" } } When used, this will include the following keys: Key Type Example cold_start bool false function_name str \"example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_memory_size int 128 function_arn str \"arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_request_id str \"899856cb-83d1-40d7-8611-9e78f15f32f4\" Logging incoming event \u00b6 When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ... Setting a Correlation ID \u00b6 New in 1.12.0 You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger () @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Appending additional keys \u00b6 You can append additional keys using either mechanism: Persist new keys across all future log messages via structure_logs method Add additional keys on a per log message basis via extra parameter structure_logs method \u00b6 You can append your own keys to your existing Logger via structure_logs(append=True, **kwargs) method. Omitting append=True will reset the existing structured logs to standard keys + keys provided as arguments collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): order_id = event . get ( \"order_id\" ) logger . structure_logs ( append = True , order_id = order_id ) logger . info ( \"Collecting payment\" ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"order_id\" : \"order_id_value\" , \"message\" : \"Collecting payment\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can use the highlighted line above as an example. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the logger. extra parameter \u00b6 New in 1.10.0 Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Hello\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2021-01-12 14:08:12,357\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"request_id\" : \"1123\" , \"message\" : \"Collecting payment\" } set_correlation_id method \u00b6 New in 1.12.0 You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger () def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Logging exceptions \u00b6 When logging exceptions, Logger will add new keys named exception_name and exception with the full traceback as a string. Tip New in 1.12.0 You can use your preferred Log Analytics tool to enumerate exceptions across all your services using exception_name key. logging_an_exception.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"level\" : \"ERROR\" , \"location\" : \"<module>:4\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2020-08-28 18:11:38,886\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" } Advanced \u00b6 Reusing Logger across your code \u00b6 Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead. Sampling debug logs \u00b6 Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can set using POWERTOOLS_LOGGER_SAMPLE_RATE env var or explicitly with sample_rate parameter: Values range from 0.0 to 1 (100%) When is this useful? Take for example a sudden increase in concurrency. When looking into logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger class initialization. This means sampling may happen significantly more or less than you expect if you have a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( sample_rate = 0.1 , level = \"INFO\" ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) if \"order_id\" in event : logger . info ( \"Collecting payment\" ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" } { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Collecting payment\" } Migrating from other Loggers \u00b6 If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions . The service parameter \u00b6 Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . Logging output example 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" } Inheriting Loggers \u00b6 Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set. Overriding Log records \u00b6 You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id , and datefmt . However, sampling_rate key is part of the specification and cannot be suppressed. xray_trace_id logging key This key is only added if X-Ray Tracing is enabled for your Lambda function. Once enabled, this key allows the integration between CloudWatch Logs and Service Lens. lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # override default values for location and timestamp format logger = Logger ( location = \"[ %(funcName)s ] %(module)s \" , datefmt = \"%m/ %d /%Y %I:%M:%S %p\" ) # suppress location key logger = Logger ( stream = stdout , location = None ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"[<module>] scratch\" , \"message\" : \"hello world\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 } Reordering log records position \u00b6 You can also change the order of the following log record keys via the log_record_order parameter: level , location , message , xray_trace_id , and timestamp lambda_handler.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( stream = stdout , log_record_order = [ \"message\" ]) # Default key sorting order # Logger(stream=stdout, log_record_order=[\"level\",\"location\",\"message\",\"timestamp\"]) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"message\" : \"hello world\" , \"level\" : \"INFO\" , \"location\" : \"[<module>]:6\" , \"timestamp\" : \"2021-02-09 09:36:12,280\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 } Testing your code \u00b6 When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context ) Pytest live log feature \u00b6 Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured). Built-in Correlation ID expressions \u00b6 New in 1.12.0 You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID FAQ \u00b6 How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between structure_log and extra ? Keys added with structure_log will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . structure_logs ( append = True , payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:5\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:6\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" }","title":"Logger"},{"location":"core/logger/#key-features","text":"Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Append additional keys to structured log at any point in time","title":"Key features"},{"location":"core/logger/#getting-started","text":"Logger requires two settings: Setting Description Environment variable Constructor parameter Logging level Sets how verbose Logger should be (INFO, by default) LOG_LEVEL level Service Sets service key that will be present across all log statements POWERTOOLS_SERVICE_NAME service Example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO POWERTOOLS_SERVICE_NAME : example app.py 1 2 3 from aws_lambda_powertools import Logger logger = Logger () # Sets service via env var # OR logger = Logger(service=\"example\")","title":"Getting started"},{"location":"core/logger/#standard-structured-keys","text":"Your Logger will include the following keys to your structured logging, by default: Key Type Example Description timestamp str \"2020-05-24 18:17:33,774\" Timestamp of actual log statement level str \"INFO\" Logging level location str \"collect.handler:1\" Source code location where statement was executed service str \"payment\" Service name defined. \"service_undefined\" will be used if unknown sampling_rate int 0.1 Debug logging sampling rate in percentage e.g. 10% in this case message any \"Collecting payment\" Log statement value. Unserializable JSON values will be casted to string xray_trace_id str \"1-5759e988-bd862e3fe1be46a994272793\" X-Ray Trace ID when Lambda function has enabled Tracing","title":"Standard structured keys"},{"location":"core/logger/#capturing-lambda-context-info","text":"You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : \"Collecting payment\" }, { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:15\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : { \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" } } When used, this will include the following keys: Key Type Example cold_start bool false function_name str \"example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_memory_size int 128 function_arn str \"arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_request_id str \"899856cb-83d1-40d7-8611-9e78f15f32f4\"","title":"Capturing Lambda context info"},{"location":"core/logger/#logging-incoming-event","text":"When debugging in non-production environments, you can instruct Logger to log the incoming event with log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ...","title":"Logging incoming event"},{"location":"core/logger/#setting-a-correlation-id","text":"New in 1.12.0 You can set a Correlation ID using correlation_id_path param by passing a JMESPath expression . collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( correlation_id_path = \"headers.my_request_id_header\" ) def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"headers\" : { \"my_request_id_header\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } We provide built-in JMESPath expressions for known event sources, where either a request ID or X-Ray Trace ID are present. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger from aws_lambda_powertools.logging import correlation_paths logger = Logger () @logger . inject_lambda_context ( correlation_id_path = correlation_paths . API_GATEWAY_REST ) def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" }","title":"Setting a Correlation ID"},{"location":"core/logger/#appending-additional-keys","text":"You can append additional keys using either mechanism: Persist new keys across all future log messages via structure_logs method Add additional keys on a per log message basis via extra parameter","title":"Appending additional keys"},{"location":"core/logger/#structure_logs-method","text":"You can append your own keys to your existing Logger via structure_logs(append=True, **kwargs) method. Omitting append=True will reset the existing structured logs to standard keys + keys provided as arguments collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): order_id = event . get ( \"order_id\" ) logger . structure_logs ( append = True , order_id = order_id ) logger . info ( \"Collecting payment\" ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"order_id\" : \"order_id_value\" , \"message\" : \"Collecting payment\" } Logger will automatically reject any key with a None value If you conditionally add keys depending on the payload, you can use the highlighted line above as an example. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the logger.","title":"structure_logs method"},{"location":"core/logger/#extra-parameter","text":"New in 1.10.0 Extra parameter is available for all log levels' methods, as implemented in the standard logging library - e.g. logger.info, logger.warning . It accepts any dictionary, and all keyword arguments will be added as part of the root structure of the logs for that log statement. Any keyword argument added using extra will not be persisted for subsequent messages. extra_parameter.py 1 2 3 4 5 logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Hello\" , extra = fields ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2021-01-12 14:08:12,357\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"request_id\" : \"1123\" , \"message\" : \"Collecting payment\" }","title":"extra parameter"},{"location":"core/logger/#set_correlation_id-method","text":"New in 1.12.0 You can set a correlation_id to your existing Logger via set_correlation_id(value) method by passing any string value. collect.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): logger . set_correlation_id ( event [ \"requestContext\" ][ \"requestId\" ]) logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" } Alternatively, you can combine Data Classes utility with Logger to use dot notation object: collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent logger = Logger () def handler ( event , context ): event = APIGatewayProxyEvent ( event ) logger . set_correlation_id ( event . request_context . request_id ) logger . info ( \"Collecting payment\" ) ... Example Event 1 2 3 4 5 { \"requestContext\" : { \"requestId\" : \"correlation_id_value\" } } Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"correlation_id\" : \"correlation_id_value\" , \"message\" : \"Collecting payment\" }","title":"set_correlation_id method"},{"location":"core/logger/#logging-exceptions","text":"When logging exceptions, Logger will add new keys named exception_name and exception with the full traceback as a string. Tip New in 1.12.0 You can use your preferred Log Analytics tool to enumerate exceptions across all your services using exception_name key. logging_an_exception.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 { \"level\" : \"ERROR\" , \"location\" : \"<module>:4\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2020-08-28 18:11:38,886\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"exception_name\" : \"ValueError\" , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" }","title":"Logging exceptions"},{"location":"core/logger/#advanced","text":"","title":"Advanced"},{"location":"core/logger/#reusing-logger-across-your-code","text":"Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () # POWERTOOLS_SERVICE_NAME: \"payment\" def handler ( event , context ): shared . inject_payment_id ( event ) ... shared.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger logger = Logger ( child = True ) # POWERTOOLS_SERVICE_NAME: \"payment\" def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event . get ( \"payment_id\" )) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Changes in either parent or child logger will be propagated bi-directionally. Child loggers will be named after the following convention {service}.{filename} If you forget to use child param but the service name is the same of the parent, we will return the existing parent Logger instead.","title":"Reusing Logger across your code"},{"location":"core/logger/#sampling-debug-logs","text":"Use sampling when you want to dynamically change your log level to DEBUG based on a percentage of your concurrent/cold start invocations . You can set using POWERTOOLS_LOGGER_SAMPLE_RATE env var or explicitly with sample_rate parameter: Values range from 0.0 to 1 (100%) When is this useful? Take for example a sudden increase in concurrency. When looking into logs you might not have enough information, and while you can adjust log levels it might not happen again. This feature takes into account transient issues where additional debugging information can be useful. Sampling decision happens at the Logger class initialization. This means sampling may happen significantly more or less than you expect if you have a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling upon every invocation, please open a feature request . collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( sample_rate = 0.1 , level = \"INFO\" ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) if \"order_id\" in event : logger . info ( \"Collecting payment\" ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" } { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Collecting payment\" }","title":"Sampling debug logs"},{"location":"core/logger/#migrating-from-other-loggers","text":"If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions .","title":"Migrating from other Loggers"},{"location":"core/logger/#the-service-parameter","text":"Service is what defines the Logger name, including what the Lambda function is responsible for, or part of (e.g payment service). For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . Logging output example 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" }","title":"The service parameter"},{"location":"core/logger/#inheriting-loggers","text":"Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) correct_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" , child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, resulting in no message being logged to standard output. Tip This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set.","title":"Inheriting Loggers"},{"location":"core/logger/#overriding-log-records","text":"You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id , and datefmt . However, sampling_rate key is part of the specification and cannot be suppressed. xray_trace_id logging key This key is only added if X-Ray Tracing is enabled for your Lambda function. Once enabled, this key allows the integration between CloudWatch Logs and Service Lens. lambda_handler.py We honour standard logging library string formats . 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # override default values for location and timestamp format logger = Logger ( location = \"[ %(funcName)s ] %(module)s \" , datefmt = \"%m/ %d /%Y %I:%M:%S %p\" ) # suppress location key logger = Logger ( stream = stdout , location = None ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 { \"level\" : \"INFO\" , \"location\" : \"[<module>] scratch\" , \"message\" : \"hello world\" , \"timestamp\" : \"02/09/2021 09:25:17 AM\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 }","title":"Overriding Log records"},{"location":"core/logger/#testing-your-code","text":"When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py Note that dataclasses are available in Python 3.7+ only. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from dataclasses import dataclass import pytest @pytest . fixture def lambda_context (): @dataclass class LambdaContext : function_name : str = \"test\" memory_limit_in_mb : int = 128 invoked_function_arn : str = \"arn:aws:lambda:eu-west-1:809313241:function:test\" aws_request_id : str = \"52fdfc07-2182-154f-163f-5f0f9a621d72\" return LambdaContext () def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } your_lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated fake_lambda_context_for_logger_py36.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections import namedtuple import pytest @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_context ): test_event = { 'test' : 'event' } # this will now have a Context object populated your_lambda_handler ( test_event , lambda_context )","title":"Testing your code"},{"location":"core/logger/#pytest-live-log-feature","text":"Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).","title":"Pytest live log feature"},{"location":"core/logger/#built-in-correlation-id-expressions","text":"New in 1.12.0 You can use any of the following built-in JMESPath expressions as part of inject_lambda_context decorator . Escaping necessary for the - character Any object key named with - must be escaped, for example request.headers.\"x-amzn-trace-id\" . Name Expression Description API_GATEWAY_REST \"requestContext.requestId\" API Gateway REST API request ID API_GATEWAY_HTTP \"requestContext.requestId\" API Gateway HTTP API request ID APPSYNC_RESOLVER 'request.headers.\"x-amzn-trace-id\"' AppSync X-Ray Trace ID APPLICATION_LOAD_BALANCER 'headers.\"x-amzn-trace-id\"' ALB X-Ray Trace ID EVENT_BRIDGE \"id\" EventBridge Event ID","title":"Built-in Correlation ID expressions"},{"location":"core/logger/#faq","text":"How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between structure_log and extra ? Keys added with structure_log will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Here's an example where we persist payment_id not request_id . Note that payment_id remains in both log messages while booking_id is only available in the first message. lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . structure_logs ( append = True , payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:5\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:6\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" }","title":"FAQ"},{"location":"core/metrics/","text":"Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF) . These metrics can be visualized through Amazon CloudWatch Console . Key features \u00b6 Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension Terminologies \u00b6 If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained Getting started \u00b6 Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory. Creating metrics \u00b6 You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience. Flushing metrics \u00b6 As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch Raising SchemaValidationError on empty metrics \u00b6 If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Nesting multiple middlewares \u00b6 When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) ... Capturing cold start metric \u00b6 You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions. Advanced \u00b6 Adding metadata \u00b6 You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" } Single metric with a different dimension \u00b6 CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ... Flushing metrics manually \u00b6 If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object )) Testing your code \u00b6 Environment variables \u00b6 Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName) Clearing metrics \u00b6 Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start yield","title":"Metrics"},{"location":"core/metrics/#key-features","text":"Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create a one off metric with a different dimension","title":"Key features"},{"location":"core/metrics/#terminologies","text":"If you're new to Amazon CloudWatch, there are two terminologies you must be aware of before using this utility: Namespace . It's the highest level container that will group multiple metrics from multiple services for a given application, for example ServerlessEcommerce . Dimensions . Metrics metadata in key-value format. They help you slice and dice metrics visualization, for example ColdStart metric by Payment service . Metric terminology, visually explained","title":"Terminologies"},{"location":"core/metrics/#getting-started","text":"Metric has two global settings that will be used across all metrics emitted: Setting Description Environment variable Constructor parameter Metric namespace Logical container where all metrics will be placed e.g. ServerlessAirline POWERTOOLS_METRICS_NAMESPACE namespace Service Optionally, sets service metric dimension across all metrics e.g. payment POWERTOOLS_SERVICE_NAME service Use your application or main service as the metric namespace to easily group all metrics Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics () # Sets metric namespace and service via env var # OR metrics = Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # Sets metric namespace, and service as a metric dimension You can initialize Metrics anywhere in your code - It'll keep track of your aggregate metrics in memory.","title":"Getting started"},{"location":"core/metrics/#creating-metrics","text":"You can create metrics using add_metric , and you can create dimensions for all your aggregate metrics using add_dimension method. Metrics 1 2 3 4 5 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Metrics with custom dimensions 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) Autocomplete Metric Units MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". Metrics overflow CloudWatch EMF supports a max of 100 metrics per batch. Metrics utility will flush all metrics when adding the 100th metric. Subsequent metrics, e.g. 101th, will be aggregated into a new EMF object, for your convenience.","title":"Creating metrics"},{"location":"core/metrics/#flushing-metrics","text":"As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that automatically with the log_metrics decorator. This decorator also validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"ExampleService\" ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"BookingConfirmation\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"BookingConfirmation\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"ExampleService\" } Metric validation If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch","title":"Flushing metrics"},{"location":"core/metrics/#raising-schemavalidationerror-on-empty-metrics","text":"If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.metrics import Metrics metrics = Metrics () @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Suppressing warning messages on empty metrics If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") .","title":"Raising SchemaValidationError on empty metrics"},{"location":"core/metrics/#nesting-multiple-middlewares","text":"When using multiple middlewares, use log_metrics as your last decorator wrapping all subsequent ones to prevent early Metric validations when code hasn't been run yet. nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools import Metrics , Tracer from aws_lambda_powertools.metrics import MetricUnit tracer = Tracer ( service = \"booking\" ) metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = MetricUnit . Count , value = 1 ) ...","title":"Nesting multiple middlewares"},{"location":"core/metrics/#capturing-cold-start-metric","text":"You can optionally capture cold start metrics with log_metrics decorator via capture_cold_start_metric param. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Metrics metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics, where you might have unrelated dimensions.","title":"Capturing cold start metric"},{"location":"core/metrics/#advanced","text":"","title":"Advanced"},{"location":"core/metrics/#adding-metadata","text":"You can add high-cardinality data as part of your Metrics log with add_metadata method. This is useful when you want to search highly contextual information along with your metrics in your logs. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" }","title":"Adding metadata"},{"location":"core/metrics/#single-metric-with-a-different-dimension","text":"CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ...","title":"Single metric with a different dimension"},{"location":"core/metrics/#flushing-metrics-manually","text":"If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object ))","title":"Flushing metrics manually"},{"location":"core/metrics/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/metrics/#environment-variables","text":"Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName)","title":"Environment variables"},{"location":"core/metrics/#clearing-metrics","text":"Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start yield","title":"Clearing metrics"},{"location":"core/tracer/","text":"Tracer is an opinionated thin wrapper for AWS X-Ray Python SDK . Key features \u00b6 Auto capture cold start as annotation, and responses or full exceptions as metadata Run functions locally with SAM CLI without code change to disable tracing Explicitly disable tracing via env var POWERTOOLS_TRACE_DISABLED=\"true\" Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray Getting started \u00b6 Permissions \u00b6 Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example Lambda handler \u00b6 You can quickly start by importing the Tracer class, initialize it outside the Lambda handler, and use capture_lambda_handler decorator. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Tracer tracer = Tracer () # Sets service via env var # OR tracer = Tracer(service=\"example\") @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... When using this capture_lambda_handler decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata Annotations & Metadata \u00b6 Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using put_annotation method. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata You can add metadata using put_metadata method. 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret ) Synchronous functions \u00b6 You can trace synchronous functions using the capture_method decorator. Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret Asynchronous and generator functions \u00b6 Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ()) Advanced \u00b6 Patching modules \u00b6 Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched ) Disabling response auto-capture \u00b6 New in 1.9.0 Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. This is commonly useful in two scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object Disabling exception auto-capture \u00b6 New in 1.10.0 Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Commonly useful in one scenario You might return sensitive information from exceptions, stack traces you might not control sensitive_data_exception.py 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" ) Tracing aiohttp requests \u00b6 Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. aiohttp_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp Escape hatch mechanism \u00b6 You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . escape_hatch_context_manager_example.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret ) Concurrent asynchronous functions \u00b6 Warning As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). concurrent_async_workaround.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ... Reusing Tracer across your code \u00b6 Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ... Testing your code \u00b6 You can safely disable Tracer when unit testing your code using POWERTOOLS_TRACE_DISABLED environment variable. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest Tips \u00b6 Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tracer"},{"location":"core/tracer/#key-features","text":"Auto capture cold start as annotation, and responses or full exceptions as metadata Run functions locally with SAM CLI without code change to disable tracing Explicitly disable tracing via env var POWERTOOLS_TRACE_DISABLED=\"true\" Support tracing async methods, generators, and context managers Auto patch supported modules by AWS X-Ray","title":"Key features"},{"location":"core/tracer/#getting-started","text":"","title":"Getting started"},{"location":"core/tracer/#permissions","text":"Before your use this utility, your AWS Lambda function must have permissions to send traces to AWS X-Ray. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example","title":"Permissions"},{"location":"core/tracer/#lambda-handler","text":"You can quickly start by importing the Tracer class, initialize it outside the Lambda handler, and use capture_lambda_handler decorator. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Tracer tracer = Tracer () # Sets service via env var # OR tracer = Tracer(service=\"example\") @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... When using this capture_lambda_handler decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata","title":"Lambda handler"},{"location":"core/tracer/#annotations-metadata","text":"Annotations are key-values associated with traces and indexed by AWS X-Ray. You can use them to filter traces and to create Trace Groups to slice and dice your transactions. Metadata are key-values also associated with traces but not indexed by AWS X-Ray. You can use them to add additional context for an operation using any native object. Annotations You can add annotations using put_annotation method. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata You can add metadata using put_metadata method. 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret )","title":"Annotations &amp; Metadata"},{"location":"core/tracer/#synchronous-functions","text":"You can trace synchronous functions using the capture_method decorator. Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret","title":"Synchronous functions"},{"location":"core/tracer/#asynchronous-and-generator-functions","text":"Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 9 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 6 7 8 9 10 11 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 5 6 7 8 9 10 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ())","title":"Asynchronous and generator functions"},{"location":"core/tracer/#advanced","text":"","title":"Advanced"},{"location":"core/tracer/#patching-modules","text":"Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched )","title":"Patching modules"},{"location":"core/tracer/#disabling-response-auto-capture","text":"New in 1.9.0 Use capture_response=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize function responses as metadata. This is commonly useful in two scenarios You might return sensitive information you don't want it to be added to your traces You might manipulate streaming objects that can be read only once ; this prevents subsequent calls from being empty sensitive_data_scenario.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def fetch_sensitive_information (): return \"sensitive_information\" @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): sensitive_information = fetch_sensitive_information () streaming_object_scenario.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object","title":"Disabling response auto-capture"},{"location":"core/tracer/#disabling-exception-auto-capture","text":"New in 1.10.0 Use capture_error=False parameter in both capture_lambda_handler and capture_method decorators to instruct Tracer not to serialize exceptions as metadata. Commonly useful in one scenario You might return sensitive information from exceptions, stack traces you might not control sensitive_data_exception.py 1 2 3 4 5 from aws_lambda_powertools import Tracer @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" )","title":"Disabling exception auto-capture"},{"location":"core/tracer/#tracing-aiohttp-requests","text":"Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. aiohttp_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp","title":"Tracing aiohttp requests"},{"location":"core/tracer/#escape-hatch-mechanism","text":"You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . escape_hatch_context_manager_example.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): with tracer . provider . in_subsegment ( '## custom subsegment' ) as subsegment : ret = some_work () subsegment . put_metadata ( 'response' , ret )","title":"Escape hatch mechanism"},{"location":"core/tracer/#concurrent-asynchronous-functions","text":"Warning As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). concurrent_async_workaround.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ...","title":"Concurrent asynchronous functions"},{"location":"core/tracer/#reusing-tracer-across-your-code","text":"Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. handler.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Tracer from payment import collect_payment tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) payment.py A new instance of Tracer will be created but will reuse the previous Tracer instance configuration, similar to a Singleton. 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_method def collect_payment ( charge_id : str ): ...","title":"Reusing Tracer across your code"},{"location":"core/tracer/#testing-your-code","text":"You can safely disable Tracer when unit testing your code using POWERTOOLS_TRACE_DISABLED environment variable. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Testing your code"},{"location":"core/tracer/#tips","text":"Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tips"},{"location":"utilities/batch/","text":"The SQS batch processing utility provides a way to handle partial failures when processing batches of messages from SQS. Key Features \u00b6 Prevent successfully processed messages being returned to SQS Simple interface for individually processing messages from a batch Build your own batch processor using the base classes Background \u00b6 When using SQS as a Lambda event source mapping, Lambda functions are triggered with a batch of messages from SQS. If your function fails to process any message from the batch, the entire batch returns to your SQS queue, and your Lambda function is triggered with the same batch one more time. With this utility, messages within a batch are handled individually - only messages that were not successfully processed are returned to the queue. Warning While this utility lowers the chance of processing messages more than once, it is not guaranteed. We recommend implementing processing logic in an idempotent manner wherever possible. More details on how Lambda works with SQS can be found in the AWS documentation Getting started \u00b6 IAM Permissions \u00b6 Before your use this utility, your AWS Lambda function must have sqs:DeleteMessageBatch permission to delete successful messages directly from the queue. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Resources : MyQueue : Type : AWS::SQS::Queue HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : example Policies : - SQSPollerPolicy : QueueName : !GetAtt MyQueue.QueueName Processing messages from SQS \u00b6 You can use either sqs_batch_processor decorator, or PartialSQSProcessor as a context manager if you'd like access to the processed results. You need to create a function to handle each record from the batch - We call it record_handler from here on. Decorator 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result Tip Any non-exception/successful return from your record handler function will instruct both decorator and context manager to queue up each individual message for deletion. If the entire batch succeeds, we let Lambda to proceed in deleting the records from the queue for cost reasons. Partial failure mechanics \u00b6 All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: Any successfully processed messages , we will delete them from the queue via sqs:DeleteMessageBatch Any unprocessed messages detected , we will raise SQSBatchProcessingError to ensure failed messages return to your SQS queue Warning You will not have accessed to the processed messages within the Lambda Handler. All processing logic will and should be performed by the record_handler function. Advanced \u00b6 Choosing between decorator and context manager \u00b6 They have nearly the same behaviour when it comes to processing messages from the batch: Entire batch has been successfully processed , where your Lambda handler returned successfully, we will let SQS delete the batch to optimize your cost Entire Batch has been partially processed successfully , where exceptions were raised within your record handler , we will: 1) Delete successfully processed messages from the queue by directly calling sqs:DeleteMessageBatch 2) Raise SQSBatchProcessingError to ensure failed messages return to your SQS queue The only difference is that PartialSQSProcessor will give you access to processed messages if you need. Accessing processed messages \u00b6 Use PartialSQSProcessor context manager to access a list of all return values from your record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result Passing custom boto3 config \u00b6 If you need to pass custom configuration such as region to the SDK, you can pass your own botocore config object to the sqs_batch_processor decorator: Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Suppressing exceptions \u00b6 If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 5 from aws_lambda_powertools.utilities.batch import sqs_batch_processor @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process () Create your own partial processor \u00b6 You can create your own partial batch processor by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() - Handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() - Called once as part of the processor initialization clean() - Teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. custom_processor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table with ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Integrating exception handling with Sentry.io \u00b6 When using Sentry.io for error monitoring, you can override failure_handler to include to capture each processing exception: Credits to Charles-Axel Dein sentry_integration.py 1 2 3 4 5 6 7 8 9 10 from typing import Tuple from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from sentry_sdk import capture_exception class SQSProcessor ( PartialSQSProcessor ): def failure_handler ( self , record : Event , exception : Tuple ) -> Tuple : # type: ignore capture_exception () # send exception to Sentry logger . exception ( \"got exception while processing SQS message\" ) return super () . failure_handler ( record , exception ) # type: ignore","title":"SQS Batch Processing"},{"location":"utilities/batch/#key-features","text":"Prevent successfully processed messages being returned to SQS Simple interface for individually processing messages from a batch Build your own batch processor using the base classes","title":"Key Features"},{"location":"utilities/batch/#background","text":"When using SQS as a Lambda event source mapping, Lambda functions are triggered with a batch of messages from SQS. If your function fails to process any message from the batch, the entire batch returns to your SQS queue, and your Lambda function is triggered with the same batch one more time. With this utility, messages within a batch are handled individually - only messages that were not successfully processed are returned to the queue. Warning While this utility lowers the chance of processing messages more than once, it is not guaranteed. We recommend implementing processing logic in an idempotent manner wherever possible. More details on how Lambda works with SQS can be found in the AWS documentation","title":"Background"},{"location":"utilities/batch/#getting-started","text":"","title":"Getting started"},{"location":"utilities/batch/#iam-permissions","text":"Before your use this utility, your AWS Lambda function must have sqs:DeleteMessageBatch permission to delete successful messages directly from the queue. Example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Resources : MyQueue : Type : AWS::SQS::Queue HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : example Policies : - SQSPollerPolicy : QueueName : !GetAtt MyQueue.QueueName","title":"IAM Permissions"},{"location":"utilities/batch/#processing-messages-from-sqs","text":"You can use either sqs_batch_processor decorator, or PartialSQSProcessor as a context manager if you'd like access to the processed results. You need to create a function to handle each record from the batch - We call it record_handler from here on. Decorator 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result Tip Any non-exception/successful return from your record handler function will instruct both decorator and context manager to queue up each individual message for deletion. If the entire batch succeeds, we let Lambda to proceed in deleting the records from the queue for cost reasons.","title":"Processing messages from SQS"},{"location":"utilities/batch/#partial-failure-mechanics","text":"All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: Any successfully processed messages , we will delete them from the queue via sqs:DeleteMessageBatch Any unprocessed messages detected , we will raise SQSBatchProcessingError to ensure failed messages return to your SQS queue Warning You will not have accessed to the processed messages within the Lambda Handler. All processing logic will and should be performed by the record_handler function.","title":"Partial failure mechanics"},{"location":"utilities/batch/#advanced","text":"","title":"Advanced"},{"location":"utilities/batch/#choosing-between-decorator-and-context-manager","text":"They have nearly the same behaviour when it comes to processing messages from the batch: Entire batch has been successfully processed , where your Lambda handler returned successfully, we will let SQS delete the batch to optimize your cost Entire Batch has been partially processed successfully , where exceptions were raised within your record handler , we will: 1) Delete successfully processed messages from the queue by directly calling sqs:DeleteMessageBatch 2) Raise SQSBatchProcessingError to ensure failed messages return to your SQS queue The only difference is that PartialSQSProcessor will give you access to processed messages if you need.","title":"Choosing between decorator and context manager"},{"location":"utilities/batch/#accessing-processed-messages","text":"Use PartialSQSProcessor context manager to access a list of all return values from your record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): return do_something_with ( record [ \"body\" ]) def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result","title":"Accessing processed messages"},{"location":"utilities/batch/#passing-custom-boto3-config","text":"If you need to pass custom configuration such as region to the SDK, you can pass your own botocore config object to the sqs_batch_processor decorator: Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result","title":"Passing custom boto3 config"},{"location":"utilities/batch/#suppressing-exceptions","text":"If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 5 from aws_lambda_powertools.utilities.batch import sqs_batch_processor @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 6 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process ()","title":"Suppressing exceptions"},{"location":"utilities/batch/#create-your-own-partial-processor","text":"You can create your own partial batch processor by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() - Handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() - Called once as part of the processor initialization clean() - Teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. custom_processor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table with ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 }","title":"Create your own partial processor"},{"location":"utilities/batch/#integrating-exception-handling-with-sentryio","text":"When using Sentry.io for error monitoring, you can override failure_handler to include to capture each processing exception: Credits to Charles-Axel Dein sentry_integration.py 1 2 3 4 5 6 7 8 9 10 from typing import Tuple from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from sentry_sdk import capture_exception class SQSProcessor ( PartialSQSProcessor ): def failure_handler ( self , record : Event , exception : Tuple ) -> Tuple : # type: ignore capture_exception () # send exception to Sentry logger . exception ( \"got exception while processing SQS message\" ) return super () . failure_handler ( record , exception ) # type: ignore","title":"Integrating exception handling with Sentry.io"},{"location":"utilities/data_classes/","text":"Event Source Data Classes utility provides classes self-describing Lambda event sources, including API decorators when applicable. Key Features \u00b6 Type hinting and code completion for common event types Helper functions for decoding/deserializing nested fields Docstrings for fields contained in event schemas Background When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function. Getting started \u00b6 Utilizing the data classes \u00b6 The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Autocomplete with self-documented properties and methods Supported event sources \u00b6 Event Source Data_class API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy event v2 APIGatewayProxyEventV2 AppSync Resolver AppSyncResolverEvent CloudWatch Logs CloudWatchLogsEvent Cognito User Pool Multiple available under cognito_user_pool_event Connect Contact Flow ConnectContactFlowEvent DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent S3 S3Event SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings). API Gateway Proxy \u00b6 It is used for either API Gateway REST API or HTTP API using v1 proxy event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) request_context = event . request_context identity = request_context . identity if 'helloworld' in event . path and event . http_method == 'GET' : user = identity . user do_something_with ( event . body , user ) API Gateway Proxy v2 \u00b6 app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEventV2 def lambda_handler ( event , context ): event : APIGatewayProxyEventV2 = APIGatewayProxyEventV2 ( event ) request_context = event . request_context query_string_parameters = event . query_string_parameters if 'helloworld' in event . raw_path and request_context . http . method == 'POST' : do_something_with ( event . body , query_string_parameters ) AppSync Resolver \u00b6 New in 1.12.0 Used when building Lambda GraphQL Resolvers with Amplify GraphQL Transform Library ( @function ), and AppSync Direct Lambda Resolvers . In this example, we also use the new Logger correlation_id and built-in correlation_paths to extract, if available, X-Ray Trace ID in AppSync request headers: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools.logging import Logger , correlation_paths from aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import ( AppSyncResolverEvent , AppSyncIdentityCognito ) logger = Logger () def get_locations ( name : str = None , size : int = 0 , page : int = 0 ): \"\"\"Your resolver logic here\"\"\" @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): event : AppSyncResolverEvent = AppSyncResolverEvent ( event ) # Case insensitive look up of request headers x_forwarded_for = event . get_header_value ( \"x-forwarded-for\" ) # Support for AppSyncIdentityCognito or AppSyncIdentityIAM identity types assert isinstance ( event . identity , AppSyncIdentityCognito ) identity : AppSyncIdentityCognito = event . identity # Logging with correlation_id logger . debug ({ \"x-forwarded-for\" : x_forwarded_for , \"username\" : identity . username }) if event . type_name == \"Merchant\" and event . field_name == \"locations\" : return get_locations ( ** event . arguments ) raise ValueError ( f \"Unsupported field resolver: { event . field_name } \" ) Example AppSync Event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"locations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 , \"name\" : \"value\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Example CloudWatch Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"level\" : \"DEBUG\" , \"location\" : \"lambda_handler:22\" , \"message\" :{ \"x-forwarded-for\" : \"127.0.0.1\" , \"username\" : \"mike\" }, \"timestamp\" : \"2021-03-10 12:38:40,062\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"cold_start\" : true , \"function_name\" : \"func_name\" , \"function_memory_size\" : 512 , \"function_arn\" : \"func_arn\" , \"function_request_id\" : \"6735a29c-c000-4ae3-94e6-1f1c934f7f94\" , \"correlation_id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" } CloudWatch Logs \u00b6 CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import CloudWatchLogsEvent from aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData def lambda_handler ( event , context ): event : CloudWatchLogsEvent = CloudWatchLogsEvent ( event ) decompressed_log : CloudWatchLogsDecodedData = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message ) Cognito User Pool \u00b6 Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent Post Confirmation Example \u00b6 app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = event . request . user_attributes do_something_with ( user_attributes ) Define Auth Challenge Example \u00b6 NOTE In this example we are modifying the wrapped dict response fields, so we need to return the json serializable wrapped event in event.raw_event This example is based on the AWS Cognito docs for Define Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import DefineAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : DefineAuthChallengeTriggerEvent = DefineAuthChallengeTriggerEvent ( event ) if ( len ( event . request . session ) == 1 and event . request . session [ 0 ] . challenge_name == \"SRP_A\" ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"PASSWORD_VERIFIER\" elif ( len ( event . request . session ) == 2 and event . request . session [ 1 ] . challenge_name == \"PASSWORD_VERIFIER\" and event . request . session [ 1 ] . challenge_result ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"CUSTOM_CHALLENGE\" elif ( len ( event . request . session ) == 3 and event . request . session [ 2 ] . challenge_name == \"CUSTOM_CHALLENGE\" and event . request . session [ 2 ] . challenge_result ): event . response . issue_tokens = True event . response . fail_authentication = False else : event . response . issue_tokens = False event . response . fail_authentication = True return event . raw_event SPR_A response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"PASSWORD_VERIFIER\" } } PASSWORD_VERIFIER success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"CUSTOM_CHALLENGE\" } } CUSTOM_CHALLENGE success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true }, { \"challengeName\" : \"CUSTOM_CHALLENGE\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : true , \"failAuthentication\" : false } } Create Auth Challenge Example \u00b6 This example is based on the AWS Cognito docs for Create Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import CreateAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : CreateAuthChallengeTriggerEvent = CreateAuthChallengeTriggerEvent ( event ) if event . request . challenge_name == \"CUSTOM_CHALLENGE\" : event . response . public_challenge_parameters = { \"captchaUrl\" : \"url/123.jpg\" } event . response . private_challenge_parameters = { \"answer\" : \"5\" } event . response . challenge_metadata = \"CAPTCHA_CHALLENGE\" return event . raw_event Verify Auth Challenge Response Example \u00b6 This example is based on the AWS Cognito docs for Verify Auth Challenge Response Lambda Trigger . app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import VerifyAuthChallengeResponseTriggerEvent def handler ( event : dict , context ) -> dict : event : VerifyAuthChallengeResponseTriggerEvent = VerifyAuthChallengeResponseTriggerEvent ( event ) event . response . answer_correct = ( event . request . private_challenge_parameters . get ( \"answer\" ) == event . request . challenge_answer ) return event . raw_event Connect Contact Flow \u00b6 New in 1.11.0 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.connect_contact_flow_event import ( ConnectContactFlowChannel , ConnectContactFlowEndpointType , ConnectContactFlowEvent , ConnectContactFlowInitiationMethod , ) def lambda_handler ( event , context ): event : ConnectContactFlowEvent = ConnectContactFlowEvent ( event ) assert event . contact_data . attributes == { \"Language\" : \"en-US\" } assert event . contact_data . channel == ConnectContactFlowChannel . VOICE assert event . contact_data . customer_endpoint . endpoint_type == ConnectContactFlowEndpointType . TELEPHONE_NUMBER assert event . contact_data . initiation_method == ConnectContactFlowInitiationMethod . API DynamoDB Streams \u00b6 The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import ( DynamoDBStreamEvent , DynamoDBRecordEventName ) def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image ) EventBridge \u00b6 app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import EventBridgeEvent def lambda_handler ( event , context ): event : EventBridgeEvent = EventBridgeEvent ( event ) do_something_with ( event . detail ) Kinesis streams \u00b6 Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes import KinesisStreamEvent def lambda_handler ( event , context ): event : KinesisStreamEvent = KinesisStreamEvent ( event ) kinesis_record = next ( event . records ) . kinesis # if data was delivered as text data = kinesis_record . data_as_text () # if data was delivered as json data = kinesis_record . data_as_json () do_something_with ( data ) S3 \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 12 from urllib.parse import unquote_plus from aws_lambda_powertools.utilities.data_classes import S3Event def lambda_handler ( event , context ): event : S3Event = S3Event ( event ) bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = unquote_plus ( record . s3 . get_object . key ) do_something_with ( f ' { bucket_name } / { object_key } ' ) SES \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SESEvent def lambda_handler ( event , context ): event : SESEvent = SESEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject ) SNS \u00b6 app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SNSEvent def lambda_handler ( event , context ): event : SNSEvent = SNSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message ) SQS \u00b6 app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes import SQSEvent def lambda_handler ( event , context ): event : SQSEvent = SQSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"Event Source Data Classes"},{"location":"utilities/data_classes/#key-features","text":"Type hinting and code completion for common event types Helper functions for decoding/deserializing nested fields Docstrings for fields contained in event schemas Background When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function.","title":"Key Features"},{"location":"utilities/data_classes/#getting-started","text":"","title":"Getting started"},{"location":"utilities/data_classes/#utilizing-the-data-classes","text":"The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class. app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) if 'helloworld' in event . path and event . http_method == 'GET' : do_something_with ( event . body , user ) Autocomplete with self-documented properties and methods","title":"Utilizing the data classes"},{"location":"utilities/data_classes/#supported-event-sources","text":"Event Source Data_class API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy event v2 APIGatewayProxyEventV2 AppSync Resolver AppSyncResolverEvent CloudWatch Logs CloudWatchLogsEvent Cognito User Pool Multiple available under cognito_user_pool_event Connect Contact Flow ConnectContactFlowEvent DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent S3 S3Event SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings).","title":"Supported event sources"},{"location":"utilities/data_classes/#api-gateway-proxy","text":"It is used for either API Gateway REST API or HTTP API using v1 proxy event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) request_context = event . request_context identity = request_context . identity if 'helloworld' in event . path and event . http_method == 'GET' : user = identity . user do_something_with ( event . body , user )","title":"API Gateway Proxy"},{"location":"utilities/data_classes/#api-gateway-proxy-v2","text":"app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEventV2 def lambda_handler ( event , context ): event : APIGatewayProxyEventV2 = APIGatewayProxyEventV2 ( event ) request_context = event . request_context query_string_parameters = event . query_string_parameters if 'helloworld' in event . raw_path and request_context . http . method == 'POST' : do_something_with ( event . body , query_string_parameters )","title":"API Gateway Proxy v2"},{"location":"utilities/data_classes/#appsync-resolver","text":"New in 1.12.0 Used when building Lambda GraphQL Resolvers with Amplify GraphQL Transform Library ( @function ), and AppSync Direct Lambda Resolvers . In this example, we also use the new Logger correlation_id and built-in correlation_paths to extract, if available, X-Ray Trace ID in AppSync request headers: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from aws_lambda_powertools.logging import Logger , correlation_paths from aws_lambda_powertools.utilities.data_classes.appsync_resolver_event import ( AppSyncResolverEvent , AppSyncIdentityCognito ) logger = Logger () def get_locations ( name : str = None , size : int = 0 , page : int = 0 ): \"\"\"Your resolver logic here\"\"\" @logger . inject_lambda_context ( correlation_id_path = correlation_paths . APPSYNC_RESOLVER ) def lambda_handler ( event , context ): event : AppSyncResolverEvent = AppSyncResolverEvent ( event ) # Case insensitive look up of request headers x_forwarded_for = event . get_header_value ( \"x-forwarded-for\" ) # Support for AppSyncIdentityCognito or AppSyncIdentityIAM identity types assert isinstance ( event . identity , AppSyncIdentityCognito ) identity : AppSyncIdentityCognito = event . identity # Logging with correlation_id logger . debug ({ \"x-forwarded-for\" : x_forwarded_for , \"username\" : identity . username }) if event . type_name == \"Merchant\" and event . field_name == \"locations\" : return get_locations ( ** event . arguments ) raise ValueError ( f \"Unsupported field resolver: { event . field_name } \" ) Example AppSync Event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"typeName\" : \"Merchant\" , \"fieldName\" : \"locations\" , \"arguments\" : { \"page\" : 2 , \"size\" : 1 , \"name\" : \"value\" }, \"identity\" : { \"claims\" : { \"iat\" : 1615366261 ... }, \"username\" : \"mike\" , ... }, \"request\" : { \"headers\" : { \"x-amzn-trace-id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" , \"x-forwarded-for\" : \"127.0.0.1\" ... } }, ... } Example CloudWatch Log 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"level\" : \"DEBUG\" , \"location\" : \"lambda_handler:22\" , \"message\" :{ \"x-forwarded-for\" : \"127.0.0.1\" , \"username\" : \"mike\" }, \"timestamp\" : \"2021-03-10 12:38:40,062\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"cold_start\" : true , \"function_name\" : \"func_name\" , \"function_memory_size\" : 512 , \"function_arn\" : \"func_arn\" , \"function_request_id\" : \"6735a29c-c000-4ae3-94e6-1f1c934f7f94\" , \"correlation_id\" : \"Root=1-60488877-0b0c4e6727ab2a1c545babd0\" }","title":"AppSync Resolver"},{"location":"utilities/data_classes/#cloudwatch-logs","text":"CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import CloudWatchLogsEvent from aws_lambda_powertools.utilities.data_classes.cloud_watch_logs_event import CloudWatchLogsDecodedData def lambda_handler ( event , context ): event : CloudWatchLogsEvent = CloudWatchLogsEvent ( event ) decompressed_log : CloudWatchLogsDecodedData = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message )","title":"CloudWatch Logs"},{"location":"utilities/data_classes/#cognito-user-pool","text":"Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent","title":"Cognito User Pool"},{"location":"utilities/data_classes/#post-confirmation-example","text":"app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = event . request . user_attributes do_something_with ( user_attributes )","title":"Post Confirmation Example"},{"location":"utilities/data_classes/#define-auth-challenge-example","text":"NOTE In this example we are modifying the wrapped dict response fields, so we need to return the json serializable wrapped event in event.raw_event This example is based on the AWS Cognito docs for Define Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import DefineAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : DefineAuthChallengeTriggerEvent = DefineAuthChallengeTriggerEvent ( event ) if ( len ( event . request . session ) == 1 and event . request . session [ 0 ] . challenge_name == \"SRP_A\" ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"PASSWORD_VERIFIER\" elif ( len ( event . request . session ) == 2 and event . request . session [ 1 ] . challenge_name == \"PASSWORD_VERIFIER\" and event . request . session [ 1 ] . challenge_result ): event . response . issue_tokens = False event . response . fail_authentication = False event . response . challenge_name = \"CUSTOM_CHALLENGE\" elif ( len ( event . request . session ) == 3 and event . request . session [ 2 ] . challenge_name == \"CUSTOM_CHALLENGE\" and event . request . session [ 2 ] . challenge_result ): event . response . issue_tokens = True event . response . fail_authentication = False else : event . response . issue_tokens = False event . response . fail_authentication = True return event . raw_event SPR_A response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"PASSWORD_VERIFIER\" } } PASSWORD_VERIFIER success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : false , \"failAuthentication\" : false , \"challengeName\" : \"CUSTOM_CHALLENGE\" } } CUSTOM_CHALLENGE success response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 { \"version\" : \"1\" , \"region\" : \"us-east-1\" , \"userPoolId\" : \"us-east-1_example\" , \"userName\" : \"UserName\" , \"callerContext\" : { \"awsSdkVersion\" : \"awsSdkVersion\" , \"clientId\" : \"clientId\" }, \"triggerSource\" : \"DefineAuthChallenge_Authentication\" , \"request\" : { \"userAttributes\" : { \"sub\" : \"4A709A36-7D63-4785-829D-4198EF10EBDA\" , \"email_verified\" : \"true\" , \"name\" : \"First Last\" , \"email\" : \"define-auth@mail.com\" }, \"session\" : [ { \"challengeName\" : \"SRP_A\" , \"challengeResult\" : true }, { \"challengeName\" : \"PASSWORD_VERIFIER\" , \"challengeResult\" : true }, { \"challengeName\" : \"CUSTOM_CHALLENGE\" , \"challengeResult\" : true } ] }, \"response\" : { \"issueTokens\" : true , \"failAuthentication\" : false } }","title":"Define Auth Challenge Example"},{"location":"utilities/data_classes/#create-auth-challenge-example","text":"This example is based on the AWS Cognito docs for Create Auth Challenge Lambda Trigger . app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import CreateAuthChallengeTriggerEvent def handler ( event : dict , context ) -> dict : event : CreateAuthChallengeTriggerEvent = CreateAuthChallengeTriggerEvent ( event ) if event . request . challenge_name == \"CUSTOM_CHALLENGE\" : event . response . public_challenge_parameters = { \"captchaUrl\" : \"url/123.jpg\" } event . response . private_challenge_parameters = { \"answer\" : \"5\" } event . response . challenge_metadata = \"CAPTCHA_CHALLENGE\" return event . raw_event","title":"Create Auth Challenge Example"},{"location":"utilities/data_classes/#verify-auth-challenge-response-example","text":"This example is based on the AWS Cognito docs for Verify Auth Challenge Response Lambda Trigger . app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import VerifyAuthChallengeResponseTriggerEvent def handler ( event : dict , context ) -> dict : event : VerifyAuthChallengeResponseTriggerEvent = VerifyAuthChallengeResponseTriggerEvent ( event ) event . response . answer_correct = ( event . request . private_challenge_parameters . get ( \"answer\" ) == event . request . challenge_answer ) return event . raw_event","title":"Verify Auth Challenge Response Example"},{"location":"utilities/data_classes/#connect-contact-flow","text":"New in 1.11.0 app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.connect_contact_flow_event import ( ConnectContactFlowChannel , ConnectContactFlowEndpointType , ConnectContactFlowEvent , ConnectContactFlowInitiationMethod , ) def lambda_handler ( event , context ): event : ConnectContactFlowEvent = ConnectContactFlowEvent ( event ) assert event . contact_data . attributes == { \"Language\" : \"en-US\" } assert event . contact_data . channel == ConnectContactFlowChannel . VOICE assert event . contact_data . customer_endpoint . endpoint_type == ConnectContactFlowEndpointType . TELEPHONE_NUMBER assert event . contact_data . initiation_method == ConnectContactFlowInitiationMethod . API","title":"Connect Contact Flow"},{"location":"utilities/data_classes/#dynamodb-streams","text":"The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes.dynamo_db_stream_event import ( DynamoDBStreamEvent , DynamoDBRecordEventName ) def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image )","title":"DynamoDB Streams"},{"location":"utilities/data_classes/#eventbridge","text":"app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import EventBridgeEvent def lambda_handler ( event , context ): event : EventBridgeEvent = EventBridgeEvent ( event ) do_something_with ( event . detail )","title":"EventBridge"},{"location":"utilities/data_classes/#kinesis-streams","text":"Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.data_classes import KinesisStreamEvent def lambda_handler ( event , context ): event : KinesisStreamEvent = KinesisStreamEvent ( event ) kinesis_record = next ( event . records ) . kinesis # if data was delivered as text data = kinesis_record . data_as_text () # if data was delivered as json data = kinesis_record . data_as_json () do_something_with ( data )","title":"Kinesis streams"},{"location":"utilities/data_classes/#s3","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 12 from urllib.parse import unquote_plus from aws_lambda_powertools.utilities.data_classes import S3Event def lambda_handler ( event , context ): event : S3Event = S3Event ( event ) bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = unquote_plus ( record . s3 . get_object . key ) do_something_with ( f ' { bucket_name } / { object_key } ' )","title":"S3"},{"location":"utilities/data_classes/#ses","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SESEvent def lambda_handler ( event , context ): event : SESEvent = SESEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject )","title":"SES"},{"location":"utilities/data_classes/#sns","text":"app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SNSEvent def lambda_handler ( event , context ): event : SNSEvent = SNSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message )","title":"SNS"},{"location":"utilities/data_classes/#sqs","text":"app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes import SQSEvent def lambda_handler ( event , context ): event : SQSEvent = SQSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"SQS"},{"location":"utilities/idempotency/","text":"Attention This utility is currently in beta . Please open an issue in GitHub for any bugs or feature requests. The idempotency utility provides a simple solution to convert your Lambda functions into idempotent operations which are safe to retry. Terminology \u00b6 The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer. Key features \u00b6 Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates Getting started \u00b6 Required resources \u00b6 Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Example using AWS Serverless Application Model (SAM) template.yml You can share a single state table for all functions New in 1.12.0 You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost. CREATE SECTION FOR PERSISTENCE LAYERS Idempotent decorator \u00b6 You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" } Choosing a payload subset for idempotency \u00b6 Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false } Idempotency request flow \u00b6 This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions. Handling exceptions \u00b6 The record in the persistence layer will be deleted if your Lambda handler returns an exception. This means that new invocations will execute again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the handler and return a successful response. Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your Lambda handler, you are not going to be able to catch it. Persistence layers \u00b6 DynamoDBPersistenceLayer \u00b6 This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) These are knobs you can use when using DynamoDB as a persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Primary key of the table. Hashed representation of the payload expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation Advanced \u00b6 Customizing the default behavior \u00b6 Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library. Handling concurrent executions with the same payload \u00b6 This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution. Using in-memory cache \u00b6 By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter. Expiring idempotency records \u00b6 Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier . Payload validation \u00b6 What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading - We provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception. Making idempotency key required \u00b6 If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, } Customizing boto configuration \u00b6 You can provide a custom boto configuration via boto_config , or an existing boto session via boto3_session parameters, when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Bring your own persistent store \u00b6 This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . DynamoDB persistence layer implementation excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key. Compatibility with other utilities \u00b6 Validation utility \u00b6 The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. You will need to account for this if you set the event_key_jmespath . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility. Extra resources \u00b6 If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Idempotency"},{"location":"utilities/idempotency/#terminology","text":"The property of idempotency means that an operation does not cause additional side effects if it is called more than once with the same input parameters. Idempotent operations will return the same result when they are called multiple times with the same parameters . This makes idempotent operations safe to retry. Idempotency key is a hash representation of either the entire event or a specific configured subset of the event, and invocation results are JSON serialized and stored in your persistence storage layer.","title":"Terminology"},{"location":"utilities/idempotency/#key-features","text":"Prevent Lambda handler from executing more than once on the same event payload during a time window Ensure Lambda handler returns the same result when called with the same payload Select a subset of the event as the idempotency key using JMESPath expressions Set a time window in which records with the same payload should be considered duplicates","title":"Key features"},{"location":"utilities/idempotency/#getting-started","text":"","title":"Getting started"},{"location":"utilities/idempotency/#required-resources","text":"Before getting started, you need to create a persistent storage layer where the idempotency utility can store its state - your lambda functions will need read and write access to it. As of now, Amazon DynamoDB is the only supported persistent storage layer, so you'll need to create a table first. Example using AWS Serverless Application Model (SAM) template.yml You can share a single state table for all functions New in 1.12.0 You can reuse the same DynamoDB table to store idempotency state. We add your function_name in addition to the idempotency key as a hash key. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Resources : IdempotencyTable : Type : AWS::DynamoDB::Table Properties : AttributeDefinitions : - AttributeName : id AttributeType : S KeySchema : - AttributeName : id KeyType : HASH TimeToLiveSpecification : AttributeName : expiration Enabled : true BillingMode : PAY_PER_REQUEST HelloWorldFunction : Type : AWS::Serverless::Function Properties : Runtime : python3.8 ... Policies : - DynamoDBCrudPolicy : TableName : !Ref IdempotencyTable Large responses with DynamoDB persistence layer When using this utility with DynamoDB, your function's responses must be smaller than 400KB . Larger items cannot be written to DynamoDB and will cause exceptions. DynamoDB Each function invocation will generally make 2 requests to DynamoDB. If the result returned by your Lambda is less than 1kb, you can expect 2 WCUs per invocation. For retried invocations, you will see 1WCU and 1RCU. Review the DynamoDB pricing documentation to estimate the cost. CREATE SECTION FOR PERSISTENCE LAYERS","title":"Required resources"},{"location":"utilities/idempotency/#idempotent-decorator","text":"You can quickly start by initializing the DynamoDBPersistenceLayer class and using it with the idempotent decorator on your lambda handler. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.idempotency import ( DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( persistence_store = persistence_layer ) def handler ( event , context ): payment = create_subscription_payment ( user = event [ 'user' ], product = event [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 , } Example event 1 2 3 4 { \"username\" : \"xyz\" , \"product_id\" : \"123456789\" }","title":"Idempotent decorator"},{"location":"utilities/idempotency/#choosing-a-payload-subset-for-idempotency","text":"Dealing with always changing payloads When dealing with a more elaborate payload, where parts of the payload always change, you should use event_key_jmespath parameter. Use IdempotencyConfig to instruct the idempotent decorator to only use a portion of your payload to verify whether a request is idempotent, and therefore it should not be retried. Payment scenario In this example, we have a Lambda handler that creates a payment for a user subscribing to a product. We want to ensure that we don't accidentally charge our customer by subscribing them more than once. Imagine the function executes successfully, but the client never receives the response due to a connection issue. It is safe to retry in this instance, as the idempotent decorator will return a previously saved response. payment.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 import json from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Treat everything under the \"body\" key # in the event json object as our payload config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): body = json . loads ( event [ 'body' ]) payment = create_subscription_payment ( user = body [ 'user' ], product = body [ 'product_id' ] ) ... return { \"payment_id\" : payment . id , \"message\" : \"success\" , \"statusCode\" : 200 } Example event 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"version\" : \"2.0\" , \"routeKey\" : \"ANY /createpayment\" , \"rawPath\" : \"/createpayment\" , \"rawQueryString\" : \"\" , \"headers\" : { \"Header1\" : \"value1\" , \"Header2\" : \"value2\" }, \"requestContext\" :{ \"accountId\" : \"123456789012\" , \"apiId\" : \"api-id\" , \"domainName\" : \"id.execute-api.us-east-1.amazonaws.com\" , \"domainPrefix\" : \"id\" , \"http\" :{ \"method\" : \"POST\" , \"path\" : \"/createpayment\" , \"protocol\" : \"HTTP/1.1\" , \"sourceIp\" : \"ip\" , \"userAgent\" : \"agent\" }, \"requestId\" : \"id\" , \"routeKey\" : \"ANY /createpayment\" , \"stage\" : \"$default\" , \"time\" : \"10/Feb/2021:13:40:43 +0000\" , \"timeEpoch\" : 1612964443723 }, \"body\" : \"{\\\"user\\\":\\\"xyz\\\",\\\"product_id\\\":\\\"123456789\\\"}\" , \"isBase64Encoded\" : false }","title":"Choosing a payload subset for idempotency"},{"location":"utilities/idempotency/#idempotency-request-flow","text":"This sequence diagram shows an example flow of what happens in the payment scenario: The client was successful in receiving the result after the retry. Since the Lambda handler was only executed once, our customer hasn't been charged twice. Note Bear in mind that the entire Lambda handler is treated as a single idempotent operation. If your Lambda handler can cause multiple side effects, consider splitting it into separate functions.","title":"Idempotency request flow"},{"location":"utilities/idempotency/#handling-exceptions","text":"The record in the persistence layer will be deleted if your Lambda handler returns an exception. This means that new invocations will execute again despite having the same payload. If you don't want the record to be deleted, you need to catch exceptions within the handler and return a successful response. Warning We will raise IdempotencyPersistenceLayerError if any of the calls to the persistence layer fail unexpectedly. As this happens outside the scope of your Lambda handler, you are not going to be able to catch it.","title":"Handling exceptions"},{"location":"utilities/idempotency/#persistence-layers","text":"","title":"Persistence layers"},{"location":"utilities/idempotency/#dynamodbpersistencelayer","text":"This persistence layer is built-in, and you can either use an existing DynamoDB table or create a new one dedicated for idempotency state (recommended). app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.idempotency import DynamoDBPersistenceLayer persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , key_attr = \"idempotency_key\" , expiry_attr = \"expires_at\" , status_attr = \"current_status\" , data_attr = \"result_data\" , validation_key_attr = \"validation_key\" , ) These are knobs you can use when using DynamoDB as a persistence layer: Parameter Required Default Description table_name Table name to store state key_attr id Primary key of the table. Hashed representation of the payload expiry_attr expiration Unix timestamp of when record expires status_attr status Stores status of the lambda execution during and after invocation data_attr data Stores results of successfully executed Lambda handlers validation_key_attr validation Hashed representation of the parts of the event used for validation","title":"DynamoDBPersistenceLayer"},{"location":"utilities/idempotency/#advanced","text":"","title":"Advanced"},{"location":"utilities/idempotency/#customizing-the-default-behavior","text":"Idempotent decorator can be further configured with IdempotencyConfig as seen in the previous example. These are the available options for further configuration Parameter Default Description event_key_jmespath \"\" JMESPath expression to extract the idempotency key from the event record payload_validation_jmespath \"\" JMESPath expression to validate whether certain parameters have changed in the event while the event payload raise_on_no_idempotency_key False Raise exception if no idempotency key was found in the request expires_after_seconds 3600 The number of seconds to wait before a record is expired use_local_cache False Whether to locally cache idempotency results local_cache_max_items 256 Max number of items to store in local cache hash_function md5 Function to use for calculating hashes, as provided by hashlib in the standard library.","title":"Customizing the default behavior"},{"location":"utilities/idempotency/#handling-concurrent-executions-with-the-same-payload","text":"This utility will raise an IdempotencyAlreadyInProgressError exception if you receive multiple invocations with the same payload while the first invocation hasn't completed yet . If you receive IdempotencyAlreadyInProgressError , you can safely retry the operation. This is a locking mechanism for correctness. Since we don't know the result from the first invocation yet, we can't safely allow another concurrent execution.","title":"Handling concurrent executions with the same payload"},{"location":"utilities/idempotency/#using-in-memory-cache","text":"By default, in-memory local caching is disabled , since we don't know how much memory you consume per invocation compared to the maximum configured in your Lambda function. This in-memory cache is local to each Lambda execution environment This means it will be effective in cases where your function's concurrency is low in comparison to the number of \"retry\" invocations with the same payload, because cache might be empty. You can enable in-memory caching with the use_local_cache parameter: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , use_local_cache = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... When enabled, the default is to cache a maximum of 256 records in each Lambda execution environment - You can change it with the local_cache_max_items parameter.","title":"Using in-memory cache"},{"location":"utilities/idempotency/#expiring-idempotency-records","text":"Note By default, we expire idempotency records after an hour (3600 seconds). In most cases, it is not desirable to store the idempotency records forever. Rather, you want to guarantee that the same payload won't be executed within a period of time. You can change this window with the expires_after_seconds parameter: app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) config = IdempotencyConfig ( event_key_jmespath = \"body\" , expires_after_seconds = 5 * 60 , # 5 minutes ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... This will mark any records older than 5 minutes as expired, and the lambda handler will be executed as normal if it is invoked with a matching payload. DynamoDB time-to-live field This utility uses expiration as the TTL field in DynamoDB, as demonstrated in the SAM example earlier .","title":"Expiring idempotency records"},{"location":"utilities/idempotency/#payload-validation","text":"What if your function is invoked with the same payload except some outer parameters have changed? Example: A payment transaction for a given productID was requested twice for the same customer, however the amount to be paid has changed in the second transaction . By default, we will return the same result as it returned before, however in this instance it may be misleading - We provide a fail fast payload validation to address this edge case. With payload_validation_jmespath , you can provide an additional JMESPath expression to specify which part of the event body should be validated against previous idempotent invocations app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[userDetail, productId]\" , payload_validation_jmespath = \"amount\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): # Creating a subscription payment is a side # effect of calling this function! payment = create_subscription_payment ( user = event [ 'userDetail' ][ 'username' ], product = event [ 'product_id' ], amount = event [ 'amount' ] ) ... return { \"message\" : \"success\" , \"statusCode\" : 200 , \"payment_id\" : payment . id , \"amount\" : payment . amount } Example Event 1 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 500 } Example Event 2 1 2 3 4 5 6 7 8 9 { \"userDetail\" : { \"username\" : \"User1\" , \"user_email\" : \"user@example.com\" }, \"productId\" : 1500 , \"charge_type\" : \"subscription\" , \"amount\" : 1 } In this example, the userDetail and productId keys are used as the payload to generate the idempotency key, as per event_key_jmespath parameter. Note If we try to send the same request but with a different amount, we will raise IdempotencyValidationError . Without payload validation, we would have returned the same result as we did for the initial request. Since we're also returning an amount in the response, this could be quite confusing for the client. By using payload_validation_jmespath=\"amount\" , we prevent this potentially confusing behavior and instead raise an Exception.","title":"Payload validation"},{"location":"utilities/idempotency/#making-idempotency-key-required","text":"If you want to enforce that an idempotency key is required, you can set raise_on_no_idempotency_key to True . This means that we will raise IdempotencyKeyError if the evaluation of event_key_jmespath is None . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) # Requires \"user\".\"uid\" and \"order_id\" to be present config = IdempotencyConfig ( event_key_jmespath = \"[user.uid, order_id]\" , raise_on_no_idempotency_key = True , ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): pass Success Event 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"BB0D045C-8878-40C8-889E-38B3CB0A61B1\" , \"name\" : \"Foo\" }, \"order_id\" : 10000 } Failure Event Notice that order_id is now accidentally within user key 1 2 3 4 5 6 7 { \"user\" : { \"uid\" : \"DE0D000E-1234-10D1-991E-EAC1DD1D52C8\" , \"name\" : \"Joe Bloggs\" , \"order_id\" : 10000 }, }","title":"Making idempotency key required"},{"location":"utilities/idempotency/#customizing-boto-configuration","text":"You can provide a custom boto configuration via boto_config , or an existing boto session via boto3_session parameters, when constructing the persistence store. Custom session 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import boto3 from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) boto3_session = boto3 . session . Session () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto3_session = boto3_session ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ... Custom config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"body\" ) boto_config = Config () persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" , boto_config = boto_config ) @idempotent ( config = config , persistence_store = persistence_layer ) def handler ( event , context ): ...","title":"Customizing boto configuration"},{"location":"utilities/idempotency/#bring-your-own-persistent-store","text":"This utility provides an abstract base class (ABC), so that you can implement your choice of persistent storage layer. You can inherit from the BasePersistenceLayer class and implement the abstract methods _get_record , _put_record , _update_record and _delete_record . DynamoDB persistence layer implementation excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 import datetime import logging from typing import Any , Dict , Optional import boto3 from botocore.config import Config from aws_lambda_powertools.utilities.idempotency import BasePersistenceLayer from aws_lambda_powertools.utilities.idempotency.exceptions import ( IdempotencyItemAlreadyExistsError , IdempotencyItemNotFoundError , ) from aws_lambda_powertools.utilities.idempotency.persistence.base import DataRecord logger = logging . getLogger ( __name__ ) class DynamoDBPersistenceLayer ( BasePersistenceLayer ): def __init__ ( self , table_name : str , key_attr : str = \"id\" , expiry_attr : str = \"expiration\" , status_attr : str = \"status\" , data_attr : str = \"data\" , validation_key_attr : str = \"validation\" , boto_config : Optional [ Config ] = None , boto3_session : Optional [ boto3 . session . Session ] = None , ): boto_config = boto_config or Config () session = boto3_session or boto3 . session . Session () self . _ddb_resource = session . resource ( \"dynamodb\" , config = boto_config ) self . table_name = table_name self . table = self . _ddb_resource . Table ( self . table_name ) self . key_attr = key_attr self . expiry_attr = expiry_attr self . status_attr = status_attr self . data_attr = data_attr self . validation_key_attr = validation_key_attr super ( DynamoDBPersistenceLayer , self ) . __init__ () def _item_to_data_record ( self , item : Dict [ str , Any ]) -> DataRecord : \"\"\" Translate raw item records from DynamoDB to DataRecord Parameters ---------- item: Dict[str, Union[str, int]] Item format from dynamodb response Returns ------- DataRecord representation of item \"\"\" return DataRecord ( idempotency_key = item [ self . key_attr ], status = item [ self . status_attr ], expiry_timestamp = item [ self . expiry_attr ], response_data = item . get ( self . data_attr ), payload_hash = item . get ( self . validation_key_attr ), ) def _get_record ( self , idempotency_key ) -> DataRecord : response = self . table . get_item ( Key = { self . key_attr : idempotency_key }, ConsistentRead = True ) try : item = response [ \"Item\" ] except KeyError : raise IdempotencyItemNotFoundError return self . _item_to_data_record ( item ) def _put_record ( self , data_record : DataRecord ) -> None : item = { self . key_attr : data_record . idempotency_key , self . expiry_attr : data_record . expiry_timestamp , self . status_attr : data_record . status , } if self . payload_validation_enabled : item [ self . validation_key_attr ] = data_record . payload_hash now = datetime . datetime . now () try : logger . debug ( f \"Putting record for idempotency key: { data_record . idempotency_key } \" ) self . table . put_item ( Item = item , ConditionExpression = f \"attribute_not_exists( { self . key_attr } ) OR { self . expiry_attr } < :now\" , ExpressionAttributeValues = { \":now\" : int ( now . timestamp ())}, ) except self . _ddb_resource . meta . client . exceptions . ConditionalCheckFailedException : logger . debug ( f \"Failed to put record for already existing idempotency key: { data_record . idempotency_key } \" ) raise IdempotencyItemAlreadyExistsError def _update_record ( self , data_record : DataRecord ): logger . debug ( f \"Updating record for idempotency key: { data_record . idempotency_key } \" ) update_expression = \"SET #response_data = :response_data, #expiry = :expiry, #status = :status\" expression_attr_values = { \":expiry\" : data_record . expiry_timestamp , \":response_data\" : data_record . response_data , \":status\" : data_record . status , } expression_attr_names = { \"#response_data\" : self . data_attr , \"#expiry\" : self . expiry_attr , \"#status\" : self . status_attr , } if self . payload_validation_enabled : update_expression += \", #validation_key = :validation_key\" expression_attr_values [ \":validation_key\" ] = data_record . payload_hash expression_attr_names [ \"#validation_key\" ] = self . validation_key_attr kwargs = { \"Key\" : { self . key_attr : data_record . idempotency_key }, \"UpdateExpression\" : update_expression , \"ExpressionAttributeValues\" : expression_attr_values , \"ExpressionAttributeNames\" : expression_attr_names , } self . table . update_item ( ** kwargs ) def _delete_record ( self , data_record : DataRecord ) -> None : logger . debug ( f \"Deleting record for idempotency key: { data_record . idempotency_key } \" ) self . table . delete_item ( Key = { self . key_attr : data_record . idempotency_key },) Danger Pay attention to the documentation for each - you may need to perform additional checks inside these methods to ensure the idempotency guarantees remain intact. For example, the _put_record method needs to raise an exception if a non-expired record already exists in the data store with a matching key.","title":"Bring your own persistent store"},{"location":"utilities/idempotency/#compatibility-with-other-utilities","text":"","title":"Compatibility with other utilities"},{"location":"utilities/idempotency/#validation-utility","text":"The idempotency utility can be used with the validator decorator. Ensure that idempotency is the innermost decorator. Warning If you use an envelope with the validator, the event received by the idempotency utility will be the unwrapped event - not the \"raw\" event Lambda was invoked with. You will need to account for this if you set the event_key_jmespath . app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validator , envelopes from aws_lambda_powertools.utilities.idempotency import ( IdempotencyConfig , DynamoDBPersistenceLayer , idempotent ) config = IdempotencyConfig ( event_key_jmespath = \"[message, username]\" ) persistence_layer = DynamoDBPersistenceLayer ( table_name = \"IdempotencyTable\" ) @validator ( envelope = envelopes . API_GATEWAY_HTTP ) @idempotent ( config = config , persistence_store = persistence_layer ) def lambda_handler ( event , context ): cause_some_side_effects ( event [ 'username' ) return { \"message\" : event [ 'message' ], \"statusCode\" : 200 } JMESPath Powertools functions are also available Built-in functions known in the validation utility like powertools_json , powertools_base64 , powertools_base64_gzip are also available to use in this utility.","title":"Validation utility"},{"location":"utilities/idempotency/#extra-resources","text":"If you're interested in a deep dive on how Amazon uses idempotency when building our APIs, check out this article .","title":"Extra resources"},{"location":"utilities/middleware_factory/","text":"Middleware factory provides a decorator factory to create your own middleware to run logic before, and after each Lambda invocation synchronously. Key features Run logic before, after, and handle exceptions Trace each middleware when requested Middleware with no params \u00b6 You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ... Middleware with params \u00b6 You can also have your own keyword arguments after the mandatory arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : field = event . get ( field , \"\" ) if field in event : event [ field ] = obfuscate ( field ) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ... Tracing middleware execution \u00b6 If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. trace_middleware_execution.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): tracer = Tracer () # Takes a copy of an existing tracer instance tracer . add_annotation ... tracer . add_metadata ... return handler ( event , context ) Tips \u00b6 Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported Testing your code \u00b6 When unit testing middlewares with trace_execution option enabled, use POWERTOOLS_TRACE_DISABLED env var to safely disable Tracer. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Middleware factory"},{"location":"utilities/middleware_factory/#middleware-with-no-params","text":"You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ...","title":"Middleware with no params"},{"location":"utilities/middleware_factory/#middleware-with-params","text":"You can also have your own keyword arguments after the mandatory arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : field = event . get ( field , \"\" ) if field in event : event [ field ] = obfuscate ( field ) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ...","title":"Middleware with params"},{"location":"utilities/middleware_factory/#tracing-middleware-execution","text":"If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. trace_middleware_execution.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): tracer = Tracer () # Takes a copy of an existing tracer instance tracer . add_annotation ... tracer . add_metadata ... return handler ( event , context )","title":"Tracing middleware execution"},{"location":"utilities/middleware_factory/#tips","text":"Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported","title":"Tips"},{"location":"utilities/middleware_factory/#testing-your-code","text":"When unit testing middlewares with trace_execution option enabled, use POWERTOOLS_TRACE_DISABLED env var to safely disable Tracer. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Testing your code"},{"location":"utilities/parameters/","text":"The parameters utility provides high-level functions to retrieve one or multiple parameter values from AWS Systems Manager Parameter Store , AWS Secrets Manager , AWS AppConfig , Amazon DynamoDB , or bring your own. Key features \u00b6 Retrieve one or multiple parameters from the underlying provider Cache parameter values for a given amount of time (defaults to 5 seconds) Transform parameter values from JSON or base 64 encoded strings Bring Your Own Parameter Store Provider Getting started \u00b6 By default, we fetch parameters from System Manager Parameter Store, secrets from Secrets Manager, and application configuration from AppConfig. IAM Permissions \u00b6 This utility requires additional permissions to work as expected. Different parameter providers require different permissions Provider Function/Method IAM Permission SSM Parameter Store get_parameter , SSMProvider.get ssm:GetParameter SSM Parameter Store get_parameters , SSMProvider.get_multiple ssm:GetParametersByPath Secrets Manager get_secret , SecretsManager.get secretsmanager:GetSecretValue DynamoDB DynamoDBProvider.get dynamodb:GetItem DynamoDB DynamoDBProvider.get_multiple dynamodb:Query App Config AppConfigProvider.get_app_config , get_app_config appconfig:GetConfiguration Fetching parameters \u00b6 You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Fetching secrets \u00b6 You can fetch secrets stored in Secrets Manager using get_secrets . secrets_manager.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" ) Fetching app configurations \u00b6 New in 1.10.0 You can fetch application configurations in AWS AppConfig using get_app_config . The following will retrieve the latest version and store it in the cache. appconfig.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" ) Advanced \u00b6 Adjusting cache TTL \u00b6 By default, we cache parameters retrieved in-memory for 5 seconds. You can adjust how long we should keep values in cache by using the param max_age , when using get() or get_multiple() methods across all providers. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" , max_age = 60 ) # 1 minute # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Always fetching the latest \u00b6 If you'd like to always ensure you fetch the latest parameter from the store regardless if already available in cache, use force_fetch param. app.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" , force_fetch = True ) Built-in provider class \u00b6 For greater flexibility such as configuring the underlying SDK client used by built-in providers, you can use their respective Provider Classes directly. This can be used to retrieve values from other regions, change the retry behavior, etc. SSMProvider \u00b6 ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example ssm_parameter_store.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False ) SecretsProvider \u00b6 secrets_manager.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" ) DynamoDBProvider \u00b6 The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure for single parameters For single parameters, you must use id as the partition key for that table. id value my-parameter my-value Example app.py With this table, the return value of dynamodb_provider.get(\"my-param\") call will be my-value . 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) DynamoDB table structure for multiple values parameters If you want to be able to retrieve multiple parameters at once sharing the same id , your table needs to contain a sort key name sk . For example, if you want to retrieve multiple parameters having my-hash-key as ID: id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, the return of dynamodb_provider.get_multiple(\"my-hash-key\") call will be a dictionary like: 1 2 3 4 5 { \"param-a\" : \"my-value-a\" , \"param-b\" : \"my-value-b\" , \"param-c\" : \"my-value-c\" } Example app_multiple_parameters.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. values = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The DynamoDB provider supports four additional arguments at initialization. These can be used if you require a custom table structure: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. Example app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" ) AppConfigProvider \u00b6 app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" ) Create your own provider \u00b6 You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: custom_provider.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters Deserializing values with transform parameter \u00b6 For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization. The transform argument is available across all providers, including the high level functions High level functions 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" ) Providers 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" ) Partial transform failures with get_multiple() \u00b6 If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters, /param/a , /param/b and /param/c , but /param/c is malformed: partial_failures.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True ) Passing additional SDK arguments \u00b6 You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. ssm_parameter_store.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration","title":"Parameters"},{"location":"utilities/parameters/#key-features","text":"Retrieve one or multiple parameters from the underlying provider Cache parameter values for a given amount of time (defaults to 5 seconds) Transform parameter values from JSON or base 64 encoded strings Bring Your Own Parameter Store Provider","title":"Key features"},{"location":"utilities/parameters/#getting-started","text":"By default, we fetch parameters from System Manager Parameter Store, secrets from Secrets Manager, and application configuration from AppConfig.","title":"Getting started"},{"location":"utilities/parameters/#iam-permissions","text":"This utility requires additional permissions to work as expected. Different parameter providers require different permissions Provider Function/Method IAM Permission SSM Parameter Store get_parameter , SSMProvider.get ssm:GetParameter SSM Parameter Store get_parameters , SSMProvider.get_multiple ssm:GetParametersByPath Secrets Manager get_secret , SecretsManager.get secretsmanager:GetSecretValue DynamoDB DynamoDBProvider.get dynamodb:GetItem DynamoDB DynamoDBProvider.get_multiple dynamodb:Query App Config AppConfigProvider.get_app_config , get_app_config appconfig:GetConfiguration","title":"IAM Permissions"},{"location":"utilities/parameters/#fetching-parameters","text":"You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" )","title":"Fetching parameters"},{"location":"utilities/parameters/#fetching-secrets","text":"You can fetch secrets stored in Secrets Manager using get_secrets . secrets_manager.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" )","title":"Fetching secrets"},{"location":"utilities/parameters/#fetching-app-configurations","text":"New in 1.10.0 You can fetch application configurations in AWS AppConfig using get_app_config . The following will retrieve the latest version and store it in the cache. appconfig.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" )","title":"Fetching app configurations"},{"location":"utilities/parameters/#advanced","text":"","title":"Advanced"},{"location":"utilities/parameters/#adjusting-cache-ttl","text":"By default, we cache parameters retrieved in-memory for 5 seconds. You can adjust how long we should keep values in cache by using the param max_age , when using get() or get_multiple() methods across all providers. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" , max_age = 60 ) # 1 minute # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" )","title":"Adjusting cache TTL"},{"location":"utilities/parameters/#always-fetching-the-latest","text":"If you'd like to always ensure you fetch the latest parameter from the store regardless if already available in cache, use force_fetch param. app.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" , force_fetch = True )","title":"Always fetching the latest"},{"location":"utilities/parameters/#built-in-provider-class","text":"For greater flexibility such as configuring the underlying SDK client used by built-in providers, you can use their respective Provider Classes directly. This can be used to retrieve values from other regions, change the retry behavior, etc.","title":"Built-in provider class"},{"location":"utilities/parameters/#ssmprovider","text":"ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example ssm_parameter_store.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False )","title":"SSMProvider"},{"location":"utilities/parameters/#secretsprovider","text":"secrets_manager.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" )","title":"SecretsProvider"},{"location":"utilities/parameters/#dynamodbprovider","text":"The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure for single parameters For single parameters, you must use id as the partition key for that table. id value my-parameter my-value Example app.py With this table, the return value of dynamodb_provider.get(\"my-param\") call will be my-value . 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) DynamoDB table structure for multiple values parameters If you want to be able to retrieve multiple parameters at once sharing the same id , your table needs to contain a sort key name sk . For example, if you want to retrieve multiple parameters having my-hash-key as ID: id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, the return of dynamodb_provider.get_multiple(\"my-hash-key\") call will be a dictionary like: 1 2 3 4 5 { \"param-a\" : \"my-value-a\" , \"param-b\" : \"my-value-b\" , \"param-c\" : \"my-value-c\" } Example app_multiple_parameters.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. values = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The DynamoDB provider supports four additional arguments at initialization. These can be used if you require a custom table structure: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. Example app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" )","title":"DynamoDBProvider"},{"location":"utilities/parameters/#appconfigprovider","text":"app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" )","title":"AppConfigProvider"},{"location":"utilities/parameters/#create-your-own-provider","text":"You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: custom_provider.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters","title":"Create your own provider"},{"location":"utilities/parameters/#deserializing-values-with-transform-parameter","text":"For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization. The transform argument is available across all providers, including the high level functions High level functions 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" ) Providers 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" )","title":"Deserializing values with transform parameter"},{"location":"utilities/parameters/#partial-transform-failures-with-get_multiple","text":"If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters, /param/a , /param/b and /param/c , but /param/c is malformed: partial_failures.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True )","title":"Partial transform failures with get_multiple()"},{"location":"utilities/parameters/#passing-additional-sdk-arguments","text":"You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. ssm_parameter_store.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration","title":"Passing additional SDK arguments"},{"location":"utilities/parser/","text":"This utility provides data parsing and deep validation using Pydantic . Key features Defines data in pure Python classes, then parse, validate and extract only what you want Built-in envelopes to unwrap, extend, and validate popular event sources payloads Enforces type hints at runtime with user-friendly errors Extra dependency Warning This will increase the overall package size by approximately 75MB due to Pydantic dependency. Install parser's extra dependencies using pip install aws-lambda-powertools[pydantic] . Defining models \u00b6 You can define models to parse incoming events by inheriting from BaseModel . hello_world_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime. Parsing events \u00b6 You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input. event_parser decorator \u00b6 Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. NOTE: This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . event_parser_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel , ValidationError from aws_lambda_powertools.utilities.typing import LambdaContext import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ items for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string parse function \u00b6 Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. parse_standalone_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" } Built-in models \u00b6 Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service extending built-in models \u00b6 You can extend them to include your own models, and yet have all other known fields parsed along the way. EventBridge example extending_builtin_models.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel Envelopes \u00b6 When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. parse_eventbridge_payload.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeModel , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeModel ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeModel as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel built-in envelopes \u00b6 Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model] bringing your own envelope \u00b6 You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model eventbridge_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope eventbridge_envelope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model Data model validation \u00b6 Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant validating fields \u00b6 Quick validation to verify whether the field message has the value of hello world . deep_data_validation.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: 1 2 message Message must be hello world! (type=value_error) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" }) validating entire model \u00b6 root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation . Advanced use cases \u00b6 Info Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values ) e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. serializing_models_exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc. FAQ \u00b6 When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: escape_hatch.py 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"Parser"},{"location":"utilities/parser/#defining-models","text":"You can define models to parse incoming events by inheriting from BaseModel . hello_world_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime.","title":"Defining models"},{"location":"utilities/parser/#parsing-events","text":"You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input.","title":"Parsing events"},{"location":"utilities/parser/#event_parser-decorator","text":"Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. NOTE: This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . event_parser_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel , ValidationError from aws_lambda_powertools.utilities.typing import LambdaContext import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ items for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string","title":"event_parser decorator"},{"location":"utilities/parser/#parse-function","text":"Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. parse_standalone_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" }","title":"parse function"},{"location":"utilities/parser/#built-in-models","text":"Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service","title":"Built-in models"},{"location":"utilities/parser/#extending-built-in-models","text":"You can extend them to include your own models, and yet have all other known fields parsed along the way. EventBridge example extending_builtin_models.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel","title":"extending built-in models"},{"location":"utilities/parser/#envelopes","text":"When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. parse_eventbridge_payload.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeModel , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeModel ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeModel as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel","title":"Envelopes"},{"location":"utilities/parser/#built-in-envelopes","text":"Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model]","title":"built-in envelopes"},{"location":"utilities/parser/#bringing-your-own-envelope","text":"You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model eventbridge_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope eventbridge_envelope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model","title":"bringing your own envelope"},{"location":"utilities/parser/#data-model-validation","text":"Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant","title":"Data model validation"},{"location":"utilities/parser/#validating-fields","text":"Quick validation to verify whether the field message has the value of hello world . deep_data_validation.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: 1 2 message Message must be hello world! (type=value_error) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" })","title":"validating fields"},{"location":"utilities/parser/#validating-entire-model","text":"root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation .","title":"validating entire model"},{"location":"utilities/parser/#advanced-use-cases","text":"Info Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values ) e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. serializing_models_exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc.","title":"Advanced use cases"},{"location":"utilities/parser/#faq","text":"When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: escape_hatch.py 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"FAQ"},{"location":"utilities/typing/","text":"This typing utility provides static typing classes that can be used to ease the development by providing the IDE type hints. LambdaContext \u00b6 The LambdaContext typing is typically used in the handler method for the Lambda function. index.py 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"Typing"},{"location":"utilities/typing/#lambdacontext","text":"The LambdaContext typing is typically used in the handler method for the Lambda function. index.py 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"LambdaContext"},{"location":"utilities/validation/","text":"This utility provides JSON Schema validation for events and responses, including JMESPath support to unwrap events before validation. Key features Validate incoming event and response JMESPath support to unwrap events before validation applies Built-in envelopes to unwrap popular event sources payloads Validating events \u00b6 You can validate inbound and outbound events using validator decorator. You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename. Validator decorator \u00b6 Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator json_schema_dict = { .. } response_json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , outbound_schema = response_json_schema_dict ) def handler ( event , context ): return event Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both. Validate function \u00b6 Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError json_schema_dict = { .. } def handler ( event , context ): try : validate ( event = event , schema = json_schema_dict ) except SchemaValidationError as e : # do something before re-raising raise return event Validating custom formats \u00b6 New in 1.10.0 JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. If you have JSON Schemas with custom formats, for example having a int64 for high precision integers, you can pass an optional validation to handle each type using formats parameter - Otherwise it'll fail validation: Example of custom integer format 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.validation import validate event = {} # some event schema_with_custom_format = {} # some JSON schema that defines a custom format custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schema_with_custom_format , formats = custom_format ) Unwrapping events prior to validation \u00b6 You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } Here is how you'd use the envelope parameter to extract the payload inside the detail key before validating: unwrapping_events.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator , validate json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = \"detail\" ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"detail\" ) return event This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload. Built-in envelopes \u00b6 This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import envelopes , validate , validator json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) return event Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data) Built-in JMESPath functions \u00b6 You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc. powertools_json function \u00b6 Use powertools_json function to decode any JSON String. This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(data)\" ) return event handler ( event = sample_event , context = {}) powertools_base64 function \u00b6 Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(powertools_base64(data))\" ) return event handler ( event = sample_event , context = {}) powertools_base64_gzip function \u00b6 Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) return event handler ( event = sample_event , context = {}) Bring your own JMESPath function \u00b6 Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. This will replace all provided built-in functions such as powertools_json , so you will no longer be able to use them . For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.validation import validate from jmespath import functions json_schema_dict = { .. } class CustomFunctions ( functions . Functions ): @functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"\" , jmespath_options =** custom_jmespath_options ) return event","title":"Validation"},{"location":"utilities/validation/#validating-events","text":"You can validate inbound and outbound events using validator decorator. You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename.","title":"Validating events"},{"location":"utilities/validation/#validator-decorator","text":"Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator json_schema_dict = { .. } response_json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , outbound_schema = response_json_schema_dict ) def handler ( event , context ): return event Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both.","title":"Validator decorator"},{"location":"utilities/validation/#validate-function","text":"Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError json_schema_dict = { .. } def handler ( event , context ): try : validate ( event = event , schema = json_schema_dict ) except SchemaValidationError as e : # do something before re-raising raise return event","title":"Validate function"},{"location":"utilities/validation/#validating-custom-formats","text":"New in 1.10.0 JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. If you have JSON Schemas with custom formats, for example having a int64 for high precision integers, you can pass an optional validation to handle each type using formats parameter - Otherwise it'll fail validation: Example of custom integer format 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.validation import validate event = {} # some event schema_with_custom_format = {} # some JSON schema that defines a custom format custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schema_with_custom_format , formats = custom_format )","title":"Validating custom formats"},{"location":"utilities/validation/#unwrapping-events-prior-to-validation","text":"You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } Here is how you'd use the envelope parameter to extract the payload inside the detail key before validating: unwrapping_events.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator , validate json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = \"detail\" ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"detail\" ) return event This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload.","title":"Unwrapping events prior to validation"},{"location":"utilities/validation/#built-in-envelopes","text":"This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import envelopes , validate , validator json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) return event Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data)","title":"Built-in envelopes"},{"location":"utilities/validation/#built-in-jmespath-functions","text":"You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc.","title":"Built-in JMESPath functions"},{"location":"utilities/validation/#powertools_json-function","text":"Use powertools_json function to decode any JSON String. This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(data)\" ) return event handler ( event = sample_event , context = {})","title":"powertools_json function"},{"location":"utilities/validation/#powertools_base64-function","text":"Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(powertools_base64(data))\" ) return event handler ( event = sample_event , context = {})","title":"powertools_base64 function"},{"location":"utilities/validation/#powertools_base64_gzip-function","text":"Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) return event handler ( event = sample_event , context = {})","title":"powertools_base64_gzip function"},{"location":"utilities/validation/#bring-your-own-jmespath-function","text":"Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. This will replace all provided built-in functions such as powertools_json , so you will no longer be able to use them . For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.validation import validate from jmespath import functions json_schema_dict = { .. } class CustomFunctions ( functions . Functions ): @functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"\" , jmespath_options =** custom_jmespath_options ) return event","title":"Bring your own JMESPath function"}]}